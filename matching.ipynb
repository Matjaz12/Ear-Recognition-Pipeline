{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Matching\n\nIn this notebook we implement and evaluate (compute rank 1 and rank 5 accuracy) the matching stage of a recognition pipeline. Database of registered individuals stores features extracted using notebook [1]. We iterate over all non-registered samples, apply the fine-tuned segmentation model [2], crop the ear from the image, extract features using MLBP and ResNet50 (pretrained on ImageNet) and finally find the closests sample in the database. ","metadata":{}},{"cell_type":"markdown","source":"**References**\n\n[1] https://github.com/Matjaz12/Ear-Recognition-Pipeline/blob/main/feature_extraction.ipynb\n\n[2] https://github.com/Matjaz12/Ear-Recognition-Pipeline/blob/main/segmentation.ipynb","metadata":{}},{"cell_type":"markdown","source":"## Setup\n\nImport dependencies, mount google drive and unzip data. ","metadata":{}},{"cell_type":"code","source":"import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom skimage import io, transform\nimport PIL\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import transforms, utils\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:00:56.566628Z","iopub.execute_input":"2023-01-06T18:00:56.567301Z","iopub.status.idle":"2023-01-06T18:00:56.578190Z","shell.execute_reply.started":"2023-01-06T18:00:56.567258Z","shell.execute_reply":"2023-01-06T18:00:56.577160Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')\n\n#FOLDERNAME = \"IBB5\"\n#assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n\n#import sys\n#DATA_PATH = '/content/drive/My Drive/{}'.format(FOLDERNAME)\n#print(DATA_PATH)\n#sys.path.append(DATA_PATH)\n\n#ROOT_PATH = \".\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!unzip \"/content/drive/My Drive/IBB5/data.zip\" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nROOT_PATH = \"/kaggle/input/eardataset/\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarClassificationDataset(Dataset):\n    def __init__(\n        self, annotations_file, image_dir=\"./images\", transform=None\n    ):\n        super().__init__()\n        self.annotations = pd.read_csv(annotations_file, delimiter=\"\\t\", dtype=\"str\")\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Fetch image\n        folder_name, file_name = np.array(self.annotations.iloc[idx, :2])\n        image_name = os.path.join(self.image_dir, f\"{folder_name}/{file_name}.png\")\n        label = f\"{folder_name}/{file_name}\"\n\n        # Read the image and covert to numpy array\n        image = np.array(PIL.Image.open(image_name))\n\n        if self.transform:\n            augmentation = self.transform(image=image)\n            image = augmentation[\"image\"]\n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:01:18.526856Z","iopub.execute_input":"2023-01-06T18:01:18.527227Z","iopub.status.idle":"2023-01-06T18:01:18.536400Z","shell.execute_reply.started":"2023-01-06T18:01:18.527196Z","shell.execute_reply":"2023-01-06T18:01:18.535344Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 8\nNUM_WORKERS = 0 # 2\nIMAGE_HEIGHT = 300 \nIMAGE_WIDTH = 300 \nPIN_MEMORY = True\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:01:21.364472Z","iopub.execute_input":"2023-01-06T18:01:21.364989Z","iopub.status.idle":"2023-01-06T18:01:21.437509Z","shell.execute_reply.started":"2023-01-06T18:01:21.364947Z","shell.execute_reply":"2023-01-06T18:01:21.436435Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from albumentations.pytorch import ToTensorV2\nimport albumentations\n\npreprocess_transform = albumentations.Compose([\n    albumentations.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n    albumentations.Normalize(mean=mean, std=std, max_pixel_value=255.0),\n    ToTensorV2(transpose_mask=True)\n])\n\ntest_dataset = EarClassificationDataset(annotations_file=f\"/kaggle/input/tempdata/annotations_test.csv\",\n                                image_dir=f\"{ROOT_PATH}/images\",\n                                transform=preprocess_transform)\n\nprint(f\"len(test_dataset): {len(test_dataset)}\")\nimage, label = test_dataset[0]\nimage.shape, label","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:01:56.367563Z","iopub.execute_input":"2023-01-06T18:01:56.368308Z","iopub.status.idle":"2023-01-06T18:01:56.401535Z","shell.execute_reply.started":"2023-01-06T18:01:56.368264Z","shell.execute_reply":"2023-01-06T18:01:56.400625Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"len(test_dataset): 3055\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(torch.Size([3, 300, 300]), '001/03')"},"metadata":{}}]},{"cell_type":"code","source":"def crop_image(image, pred_mask):\n    image = image.to(\"cpu\").detach()\n    pred_mask = pred_mask.to(\"cpu\").detach()\n\n    image = image[0].numpy()\n    image = image.transpose((1, 2, 0))\n\n    pred_mask = pred_mask[0].numpy()\n    pred_mask = pred_mask.squeeze(axis=0)\n\n    # Compute masked image\n    pred_mask = np.stack((pred_mask, ) * 3, axis=-1)\n    masked_image = image * pred_mask\n\n    # Compute cropped image\n    nonzero_rows, nonzero_cols = np.nonzero(masked_image.mean(axis=2))\n    seg_y_start, seg_y_end = nonzero_rows.min(), nonzero_rows.max() + 1\n    seg_x_start, seg_x_end = nonzero_cols.min(), nonzero_cols.max() + 1\n\n    cropped_image = masked_image[seg_y_start:seg_y_end, seg_x_start:seg_x_end, :]\n\n    return cropped_image\n\n\ndef plot_image_and_cropped_ear(image, pred_mask, cropped_image):\n    image = image[0].to(\"cpu\").detach()\n    pred_mask = pred_mask[0].to(\"cpu\").detach()\n\n    if torch.is_tensor(image):\n        image = image.numpy()\n        image = image.transpose((1, 2, 0))\n\n    if torch.is_tensor(pred_mask):\n        pred_mask = pred_mask.numpy()\n        pred_mask = pred_mask.squeeze(axis=0)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    ax1.imshow(image)\n    ax1.imshow(pred_mask, alpha=0.6 * (pred_mask > 0))\n    ax2.imshow(cropped_image)\n    plt.show()\n\n\ndef plot_predictions(images, masks, pred_masks, n_samples=3):\n    images = images.to(\"cpu\").detach()\n    masks = masks.to(\"cpu\").detach()\n    pred_masks = pred_masks.to(\"cpu\").detach()\n\n    for idx, (image, mask, pred_mask) in enumerate(zip(images, masks, pred_masks)):\n        if idx == n_samples:\n            break\n\n        if torch.is_tensor(image):\n            image = image.numpy()\n            image = image.transpose((1, 2, 0))\n\n        if torch.is_tensor(mask):\n            mask = mask.numpy()\n            mask = mask.squeeze(axis=0)\n\n        if torch.is_tensor(pred_mask):\n            pred_mask = pred_mask.numpy()\n            pred_mask = pred_mask.squeeze(axis=0)\n\n        plt.figure(figsize=(6, 6))\n        plt.imshow(image)\n        # plt.imshow(mask, alpha=0.6 * (mask > 0))\n        plt.imshow(pred_mask, alpha=0.6 * (pred_mask > 0))\n        plt.show()\n        \n        \ndef de_normalize(images, mean, std):\n    if isinstance(mean, list):\n        mean = torch.tensor(mean).reshape((3, 1, 1)).to(DEVICE)\n        \n    if isinstance(std, list):\n        std = torch.tensor(std).reshape((3, 1, 1)).to(DEVICE)\n    \n    images = images.to(DEVICE)\n    images = images * std + mean\n    \n    return images\n\n\ndef normalize(images, mean, std):\n    if isinstance(mean, list):\n        mean = torch.tensor(mean).reshape((3, 1, 1)).to(DEVICE)\n        \n    if isinstance(std, list):\n        std = torch.tensor(std).reshape((3, 1, 1)).to(DEVICE)\n    \n    images = images.to(DEVICE)\n    images = (images - mean) / std\n    \n    return images","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:02:12.537362Z","iopub.execute_input":"2023-01-06T18:02:12.537930Z","iopub.status.idle":"2023-01-06T18:02:12.556827Z","shell.execute_reply.started":"2023-01-06T18:02:12.537892Z","shell.execute_reply":"2023-01-06T18:02:12.555434Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Recognition Pipeline","metadata":{}},{"cell_type":"markdown","source":"### Load ResNet50","metadata":{}},{"cell_type":"code","source":"#from torchvision.models import resnet50, ResNet50_Weights\n#resnet50_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\nfrom torchvision.models import resnet50\nresnet50_model = resnet50(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:02:15.030152Z","iopub.execute_input":"2023-01-06T18:02:15.030542Z","iopub.status.idle":"2023-01-06T18:02:27.018404Z","shell.execute_reply.started":"2023-01-06T18:02:15.030487Z","shell.execute_reply":"2023-01-06T18:02:27.017060Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6545382e0743499791671883863fba5c"}},"metadata":{}}]},{"cell_type":"code","source":"resnet50_feature_extractor = torch.nn.Sequential(*list(resnet50_model.children())[:-1])","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:02:28.828367Z","iopub.execute_input":"2023-01-06T18:02:28.829141Z","iopub.status.idle":"2023-01-06T18:02:28.834870Z","shell.execute_reply.started":"2023-01-06T18:02:28.829103Z","shell.execute_reply":"2023-01-06T18:02:28.833739Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"###  Load Finetuned DeepLabV3 model","metadata":{}},{"cell_type":"code","source":"from torchvision.models.segmentation.deeplabv3 import DeepLabHead\nimport torchvision\n\ndef get_model(out_channels=1, train_mode=False, pretrained=True):\n    \"\"\"\n    DeepLabv3 class with a custom head.\n    \"\"\"\n    # Load the pretrained model\n    model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=pretrained)\n\n    # Replace the classifier module with a new head\n    model.classifier = DeepLabHead(2048, out_channels)\n\n    # Set model in appropriate mode\n    model.train() if train_mode else model.eval()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:02:31.074887Z","iopub.execute_input":"2023-01-06T18:02:31.075251Z","iopub.status.idle":"2023-01-06T18:02:31.081709Z","shell.execute_reply.started":"2023-01-06T18:02:31.075221Z","shell.execute_reply":"2023-01-06T18:02:31.080555Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(\"Loading model...\")\nMODELS_DIR = \"./DeepLabV3_v1\"\nMODELS_DIR = \"/kaggle/input/seg-model/DeepLabV3_v1\"\ndeeplabv3_model = get_model(out_channels=1, train_mode=False, pretrained=True)\ndeeplabv3_model.load_state_dict(torch.load(MODELS_DIR))","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:02:31.846778Z","iopub.execute_input":"2023-01-06T18:02:31.847577Z","iopub.status.idle":"2023-01-06T18:02:53.907442Z","shell.execute_reply.started":"2023-01-06T18:02:31.847534Z","shell.execute_reply":"2023-01-06T18:02:53.906504Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/161M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d0d7df93e04a9bb8303810e21a1994"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Local Binary Patterns","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport numpy.typing as npt\nfrom skimage import feature\nfrom tqdm import tqdm\nfrom sympy import divisors\n\n\ndef compute_concatenated_histogram(image, tile_size, num_bins):\n    \"\"\"\n    Function computes a concatenated histogram, by spliting the image\n    into tiles of size `tile_size` and computing a local histogram on each tile.\n    Histograms are normalized and concatenated together.\n\n    :param image: Input image\n    :param tile_size: Size of each tile (Width, Height)\n    :param num_bins: Number of bins used in computation of local histogram.\n    :return: Concatenated histogram.\n    \"\"\"\n\n    height, width = image.shape\n    tile_height, tile_width = tile_size\n    fv = []\n\n    # Split image into tiles of size (tile_height x tile_width)\n    tiled_image = image.reshape(height // tile_height,\n                                tile_height,\n                                width // tile_width,\n                                tile_width)\n\n    tiled_image = tiled_image.swapaxes(1, 2)\n    tiled_image = tiled_image.reshape(tiled_image.shape[0] * tiled_image.shape[1], tile_height, tile_width)\n\n    # Compute histogram for each tile and concatenate them\n    for tile in tiled_image:\n        from scipy.sparse import csr_matrix\n        tile_hist, _ = np.histogram(tile, density=True, bins=num_bins, range=(0, num_bins))\n        fv.extend(tile_hist)\n\n    # Normalize the concatenated histogram\n    fv = np.array(fv)\n    # fv = fv / np.sum(fv)\n\n    return fv\n\n\nclass ScikitLBP:\n    def __init__(self, num_points: int = 8, radius: int = 1, to_hist: bool = True, method: str = \"default\"):\n        self.num_points = num_points\n        self.radius = radius\n        self.to_hist = to_hist\n        self.method = method\n\n    def describe(self, images: npt.NDArray, *args, **kwargs) -> npt.NDArray:\n        \"\"\"\n        Function computes a feature vector for each image.\n\n        :param images: A set of images.\n        :return: A set of feature vectors.\n        \"\"\"\n\n        fvs = []\n\n        if self.to_hist:\n            image_size_divisors = list(divisors(images[0].shape[0], generator=True))[1: -1]\n            tile_width = kwargs.get(\"tile_width\", image_size_divisors[len(image_size_divisors) // 2])\n            # print(f\"tile width: {tile_width}\")\n            # print(f\"number of tiles: {(images[0].shape[0] / tile_width) ** 2}\")\n\n        # Extract features for each image\n        for image in images:\n            image_lbp = feature.local_binary_pattern(image, self.num_points, self.radius, method=self.method)\n\n            # Compute feature vector\n            if self.to_hist:\n                num_bins = int(image_lbp.max()) + 1\n                fv = compute_concatenated_histogram(image_lbp, tile_size=(tile_width, tile_width),\n                                                    num_bins=num_bins)\n            else:\n                fv = image_lbp.flatten()\n\n            # Store feature vector\n            fvs.append(fv)\n\n        fvs = np.array(fvs)\n        return fvs\n\n    def __str__(self):\n        return \"ScikitLBP\"","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:02:56.159239Z","iopub.execute_input":"2023-01-06T18:02:56.159635Z","iopub.status.idle":"2023-01-06T18:02:56.550171Z","shell.execute_reply.started":"2023-01-06T18:02:56.159602Z","shell.execute_reply":"2023-01-06T18:02:56.549198Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"def load_database(database_dir):\n    database = {}\n\n    print(f\"Loading database: {database_dir}\")\n    for (dirpath, dirnames, filenames) in os.walk(database_dir):\n        for filename in filenames:\n            if filename.endswith('.pickle'):\n                full_path = os.sep.join([dirpath, filename])\n                identity = \"/\".join(full_path.strip(\".pickle\").split(\"/\")[-2:])\n                \n                with open(full_path, \"rb\") as handle:\n                    db_fv = pickle.load(handle)\n\n                database[identity] = db_fv\n                \n    return database","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUT_IMAGE_HEIGHT, OUT_IMAGE_WIDTH = 128, 128\n\ndef segment_ear(model, image, threshold):\n    image = image.unsqueeze(dim=0).to(DEVICE)\n    model = model.to(DEVICE)\n\n    # Predict a segmentation mask and apply threshold\n    pred_mask = torch.sigmoid(model(image)[\"out\"])        \n    pred_mask[pred_mask >= threshold] = 1.0\n    pred_mask[pred_mask < threshold] = 0.0\n    \n    # check if empty mask\n    if not pred_mask.nonzero().sum().item():\n        return None\n\n    # Crop image using predicted seg. mask & rescale the image\n    cropped_image = crop_image(de_normalize(image, mean, std), pred_mask)\n    cropped_image = Image.fromarray(np.uint8(cropped_image * 255)).resize((OUT_IMAGE_HEIGHT, OUT_IMAGE_WIDTH))\n\n    return cropped_image\n\n\ndef resnet_50_features(feature_extractor, image):\n    # Convert image to tensor\n    image_c = np.array(image).transpose((2, 0, 1))\n    image_c = torch.from_numpy(image_c)\n\n    # Normalize image\n    image_c = (image_c / 255.0).unsqueeze(dim=0)\n    image_c = normalize(image_c, mean, std)\n\n    # Move data and model on the device\n    feature_extractor = feature_extractor.to(DEVICE)\n    image_c = image_c.to(DEVICE)\n\n    feature_vect = feature_extractor(image_c).flatten()\n\n    return feature_vect","metadata":{"execution":{"iopub.status.busy":"2023-01-06T19:11:43.147159Z","iopub.execute_input":"2023-01-06T19:11:43.147536Z","iopub.status.idle":"2023-01-06T19:11:43.157837Z","shell.execute_reply.started":"2023-01-06T19:11:43.147482Z","shell.execute_reply":"2023-01-06T19:11:43.156626Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"import os\nfrom scipy.spatial import distance\n\ndef matching(fv, db, dist_metric=\"euclidean\", n_closest=1):\n    # Keep track of distances to samples (i.e store a dict: <sample: distance>)\n    dist_db = {}\n    \n    # Compute distance between fv and all samples in the database\n    print(\"Computing distance matrix...\")\n    dist_vector = distance.cdist(fv, np.array(db.values()))\n                    \n    # Sort distances\n    dist_db_sorted = dict(sorted(dist_db.items(), key=lambda x: x[1]))\n    n_closest_samples = np.array(list(dist_db_sorted.keys())[0:n_closest])\n    n_closest_dists = np.array(list(dist_db_sorted.values())[0:n_closest])\n    \n    return n_closest_samples, n_closest_dists\n\n\ndef rec_pipeline(\n    test_image, seg_model, lbp_descriptor, resnet_model=None, \n    database_dir=None, n_closest=1, dist_metric=\"euclidean\"\n):\n    # Segment ear from the image\n    cropped_image = segment_ear(seg_model, test_image, THRESHOLD)\n    #print(np.array(cropped_image).shape)\n    \n    if not cropped_image:\n        return None\n    \n    # Convert to gray scale\n    cropped_gray_image = np.array(cropped_image.convert(\"L\"))\n    \n    # Extract local binary patterns\n    fv = lbp_descriptor.describe(cropped_gray_image[np.newaxis, :, :], tile_width=TILE_WIDTH)[0]\n    \n    # ResNet50 features\n    if resnet_model:\n        resnet_fv = resnet_50_features(resnet_model, cropped_image).cpu()\n        fv = np.concatenate([fv, resnet_fv])\n    \n    # Find the closest samples in the database\n    closest_samples, closest_dists = matching(fv, database_dir, dist_metric, n_closest)\n    \n    return closest_samples, closest_dists","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRESHOLD = 0.25\n\nNUM_POINTS = 8\nRADIUS = 1\nTO_HIST = True\nTILE_WIDTH = 16\nlbp_descriptor = ScikitLBP(num_points=NUM_POINTS, radius=RADIUS, to_hist=TO_HIST, method=\"default\")\n\nLBP_FEATURES_DIR = \"/kaggle/input/ear-features/lbp_features1\"\nLBP_RESNET_FEATURES_DIR = \"/kaggle/input/ear-features/lbp_resnet_features1\"\n\nN_CLOSEST = 1\ndb = load_database(LBP_FEATURES_DIR)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T19:13:13.764948Z","iopub.execute_input":"2023-01-06T19:13:13.766190Z","iopub.status.idle":"2023-01-06T19:13:19.482190Z","shell.execute_reply.started":"2023-01-06T19:13:13.766144Z","shell.execute_reply":"2023-01-06T19:13:19.481179Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stdout","text":"Loading database: /kaggle/input/ear-features/lbp_features1\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    for idx, (test_image, test_label) in enumerate(tqdm(test_dataset)):\n        closest_samples, closest_dists = rec_pipeline(test_image, deeplabv3_model, lbp_descriptor, \n                                                      resnet_model=None, db=db, n_closest=N_CLOSEST)\n        print(f\"closest_samples: {closest_samples}\")\n        print(f\"test_label: {test_label}\")\n        \n        break","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:49:34.791194Z","iopub.execute_input":"2023-01-06T18:49:34.791597Z","iopub.status.idle":"2023-01-06T18:49:40.956932Z","shell.execute_reply.started":"2023-01-06T18:49:34.791564Z","shell.execute_reply":"2023-01-06T18:49:40.955923Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"  0%|          | 0/3055 [00:06<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"closest_samples: ['460']\ntest_label: 001/03\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"THRESHOLD = 0.25\n\nNUM_POINTS = 8\nRADIUS = 1\nTO_HIST = True\nTILE_WIDTH = 16\nlbp_descriptor = ScikitLBP(num_points=NUM_POINTS, radius=RADIUS, to_hist=TO_HIST, method=\"default\")\n\nLBP_FEATURES_DIR = \"/kaggle/input/ear-features/lbp_features1\"\nLBP_RESNET_FEATURES_DIR = \"/kaggle/input/ear-features/lbp_resnet_features1\"\n\nN_CLOSEST = 1\nn_correct = 0\n\nwith torch.no_grad():\n    for idx, (test_image, test_label) in enumerate(tqdm(test_dataset)):\n        closest_samples, closest_dists = rec_pipeline(test_image, deeplabv3_model, lbp_descriptor, \n                                                      resnet_model=None, database_dir=LBP_FEATURES_DIR, n_closest=N_CLOSEST)\n        \n        actual_identitiy = test_label.split(\"/\")[0]\n        #print(f\"closest_samples: {closest_samples}\")\n        #print(f\"actual_identitiy: {actual_identitiy}\")\n        \n        if actual_identitiy in closest_samples:\n            n_correct += 1\n            \nprint(f\"rank {len(closest_samples)}: {n_correct / len(test_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-06T18:52:49.537805Z","iopub.execute_input":"2023-01-06T18:52:49.538393Z","iopub.status.idle":"2023-01-06T19:03:02.922399Z","shell.execute_reply.started":"2023-01-06T18:52:49.538346Z","shell.execute_reply":"2023-01-06T19:03:02.920770Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stderr","text":"  3%|▎         | 102/3055 [10:13<4:55:56,  6.01s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1995480483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         closest_samples, closest_dists = rec_pipeline(test_image, deeplabv3_model, lbp_descriptor, \n\u001b[0;32m---> 18\u001b[0;31m                                                       resnet_model=None, database_dir=LBP_FEATURES_DIR, n_closest=N_CLOSEST)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mactual_identitiy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/262213547.py\u001b[0m in \u001b[0;36mrec_pipeline\u001b[0;34m(test_image, seg_model, lbp_descriptor, resnet_model, database_dir, n_closest, dist_metric)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Find the closest samples in the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mclosest_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosest_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_closest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclosest_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosest_dists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/2188260103.py\u001b[0m in \u001b[0;36mmatching\u001b[0;34m(fv, database_dir, dist_metric, n_closest)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0mdb_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0;31m#print(db_fv.shape, fv.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-01-06T19:10:11.646473Z","iopub.execute_input":"2023-01-06T19:10:11.647458Z","iopub.status.idle":"2023-01-06T19:10:11.655077Z","shell.execute_reply.started":"2023-01-06T19:10:11.647420Z","shell.execute_reply":"2023-01-06T19:10:11.654044Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-01-06T19:10:11.898140Z","iopub.execute_input":"2023-01-06T19:10:11.898842Z","iopub.status.idle":"2023-01-06T19:10:17.644458Z","shell.execute_reply.started":"2023-01-06T19:10:11.898798Z","shell.execute_reply":"2023-01-06T19:10:17.643432Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stdout","text":"Loading database: /kaggle/input/ear-features/lbp_features1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-01-06T19:10:34.102189Z","iopub.execute_input":"2023-01-06T19:10:34.102589Z","iopub.status.idle":"2023-01-06T19:10:34.108950Z","shell.execute_reply.started":"2023-01-06T19:10:34.102555Z","shell.execute_reply":"2023-01-06T19:10:34.107851Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"11201\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}