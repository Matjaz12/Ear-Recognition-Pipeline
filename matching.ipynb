{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea95da65",
   "metadata": {
    "papermill": {
     "duration": 0.018545,
     "end_time": "2023-01-08T15:20:40.272243",
     "exception": false,
     "start_time": "2023-01-08T15:20:40.253698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Matching\n",
    "\n",
    "In this notebook we implement and evaluate (compute rank 1 and rank 5 accuracy) the matching stage of a recognition pipeline. Database of registered individuals stores features extracted using notebook [1]. We iterate over all non-registered samples, apply the fine-tuned segmentation model [2], crop the ear from the image, extract features using MLBP and ResNet50 (pretrained on ImageNet) and finally find the closests sample in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efdab05",
   "metadata": {
    "papermill": {
     "duration": 0.014479,
     "end_time": "2023-01-08T15:20:40.305358",
     "exception": false,
     "start_time": "2023-01-08T15:20:40.290879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**References**\n",
    "\n",
    "[1] https://github.com/Matjaz12/Ear-Recognition-Pipeline/blob/main/feature_extraction.ipynb\n",
    "\n",
    "[2] https://github.com/Matjaz12/Ear-Recognition-Pipeline/blob/main/segmentation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520b7f1",
   "metadata": {
    "papermill": {
     "duration": 0.013412,
     "end_time": "2023-01-08T15:20:40.330583",
     "exception": false,
     "start_time": "2023-01-08T15:20:40.317171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import dependencies, mount google drive and unzip data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c1d244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:20:40.355619Z",
     "iopub.status.busy": "2023-01-08T15:20:40.355086Z",
     "iopub.status.idle": "2023-01-08T15:20:43.474116Z",
     "shell.execute_reply": "2023-01-08T15:20:43.473180Z"
    },
    "papermill": {
     "duration": 3.13472,
     "end_time": "2023-01-08T15:20:43.476785",
     "exception": false,
     "start_time": "2023-01-08T15:20:40.342065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skimage import io, transform\n",
    "import PIL\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, utils\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e860cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:20:43.494502Z",
     "iopub.status.busy": "2023-01-08T15:20:43.493837Z",
     "iopub.status.idle": "2023-01-08T15:20:43.498263Z",
     "shell.execute_reply": "2023-01-08T15:20:43.497278Z"
    },
    "papermill": {
     "duration": 0.01536,
     "end_time": "2023-01-08T15:20:43.500418",
     "exception": false,
     "start_time": "2023-01-08T15:20:43.485058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#FOLDERNAME = \"IBB5\"\n",
    "#assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "#import sys\n",
    "#DATA_PATH = '/content/drive/My Drive/{}'.format(FOLDERNAME)\n",
    "#print(DATA_PATH)\n",
    "#sys.path.append(DATA_PATH)\n",
    "\n",
    "#ROOT_PATH = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8df3f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:20:43.517326Z",
     "iopub.status.busy": "2023-01-08T15:20:43.516703Z",
     "iopub.status.idle": "2023-01-08T15:20:43.521174Z",
     "shell.execute_reply": "2023-01-08T15:20:43.520348Z"
    },
    "papermill": {
     "duration": 0.014953,
     "end_time": "2023-01-08T15:20:43.523065",
     "exception": false,
     "start_time": "2023-01-08T15:20:43.508112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip \"/content/drive/My Drive/IBB5/data.zip\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "ROOT_PATH = \"/kaggle/input/eardataset/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41153637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:05.516332Z",
     "iopub.status.busy": "2023-01-08T15:21:05.514843Z",
     "iopub.status.idle": "2023-01-08T15:21:05.523629Z",
     "shell.execute_reply": "2023-01-08T15:21:05.522731Z"
    },
    "papermill": {
     "duration": 0.132456,
     "end_time": "2023-01-08T15:21:05.525715",
     "exception": false,
     "start_time": "2023-01-08T15:21:05.393259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, annotations_file, image_dir=\"./images\", transform=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.annotations = pd.read_csv(annotations_file, delimiter=\"\\t\", dtype=\"str\")\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Fetch image\n",
    "        folder_name, file_name = np.array(self.annotations.iloc[idx, :2])\n",
    "        image_name = os.path.join(self.image_dir, f\"{folder_name}/{file_name}.png\")\n",
    "        label = f\"{folder_name}/{file_name}\"\n",
    "\n",
    "        # Read the image and covert to numpy array\n",
    "        image = np.array(PIL.Image.open(image_name))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentation = self.transform(image=image)\n",
    "            image = augmentation[\"image\"]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0e1a1",
   "metadata": {
    "papermill": {
     "duration": 0.114834,
     "end_time": "2023-01-08T15:21:05.754085",
     "exception": false,
     "start_time": "2023-01-08T15:21:05.639251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Copy over the hyper-parameters used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9051915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:05.990428Z",
     "iopub.status.busy": "2023-01-08T15:21:05.989832Z",
     "iopub.status.idle": "2023-01-08T15:21:06.055908Z",
     "shell.execute_reply": "2023-01-08T15:21:06.054974Z"
    },
    "papermill": {
     "duration": 0.185945,
     "end_time": "2023-01-08T15:21:06.057879",
     "exception": false,
     "start_time": "2023-01-08T15:21:05.871934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0 # 2\n",
    "IMAGE_HEIGHT = 300 \n",
    "IMAGE_WIDTH = 300 \n",
    "PIN_MEMORY = True\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce8718",
   "metadata": {
    "papermill": {
     "duration": 0.112433,
     "end_time": "2023-01-08T15:21:06.284781",
     "exception": false,
     "start_time": "2023-01-08T15:21:06.172348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Load the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd190e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:06.512655Z",
     "iopub.status.busy": "2023-01-08T15:21:06.512095Z",
     "iopub.status.idle": "2023-01-08T15:21:07.527441Z",
     "shell.execute_reply": "2023-01-08T15:21:07.526463Z"
    },
    "papermill": {
     "duration": 1.131193,
     "end_time": "2023-01-08T15:21:07.529494",
     "exception": false,
     "start_time": "2023-01-08T15:21:06.398301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset): 3055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 300, 300]), '001/03')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations\n",
    "\n",
    "preprocess_transform = albumentations.Compose([\n",
    "    albumentations.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
    "    albumentations.Normalize(mean=mean, std=std, max_pixel_value=255.0),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "test_dataset = EarClassificationDataset(annotations_file=f\"/kaggle/input/tempdata/annotations_test.csv\",\n",
    "                                image_dir=f\"{ROOT_PATH}/images\",\n",
    "                                transform=preprocess_transform)\n",
    "\n",
    "print(f\"len(test_dataset): {len(test_dataset)}\")\n",
    "image, label = test_dataset[0]\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a81f3ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:07.759675Z",
     "iopub.status.busy": "2023-01-08T15:21:07.758759Z",
     "iopub.status.idle": "2023-01-08T15:21:07.777363Z",
     "shell.execute_reply": "2023-01-08T15:21:07.776517Z"
    },
    "papermill": {
     "duration": 0.135829,
     "end_time": "2023-01-08T15:21:07.779322",
     "exception": false,
     "start_time": "2023-01-08T15:21:07.643493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_image(image, pred_mask):\n",
    "    image = image.to(\"cpu\").detach()\n",
    "    pred_mask = pred_mask.to(\"cpu\").detach()\n",
    "\n",
    "    image = image[0].numpy()\n",
    "    image = image.transpose((1, 2, 0))\n",
    "\n",
    "    pred_mask = pred_mask[0].numpy()\n",
    "    pred_mask = pred_mask.squeeze(axis=0)\n",
    "\n",
    "    # Compute masked image\n",
    "    pred_mask = np.stack((pred_mask, ) * 3, axis=-1)\n",
    "    masked_image = image * pred_mask\n",
    "\n",
    "    # Compute cropped image\n",
    "    nonzero_rows, nonzero_cols = np.nonzero(masked_image.mean(axis=2))\n",
    "    seg_y_start, seg_y_end = nonzero_rows.min(), nonzero_rows.max() + 1\n",
    "    seg_x_start, seg_x_end = nonzero_cols.min(), nonzero_cols.max() + 1\n",
    "\n",
    "    cropped_image = masked_image[seg_y_start:seg_y_end, seg_x_start:seg_x_end, :]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def plot_image_and_cropped_ear(image, pred_mask, cropped_image):\n",
    "    image = image[0].to(\"cpu\").detach()\n",
    "    pred_mask = pred_mask[0].to(\"cpu\").detach()\n",
    "\n",
    "    if torch.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "        image = image.transpose((1, 2, 0))\n",
    "\n",
    "    if torch.is_tensor(pred_mask):\n",
    "        pred_mask = pred_mask.numpy()\n",
    "        pred_mask = pred_mask.squeeze(axis=0)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(image)\n",
    "    ax1.imshow(pred_mask, alpha=0.6 * (pred_mask > 0))\n",
    "    ax2.imshow(cropped_image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions(images, masks, pred_masks, n_samples=3):\n",
    "    images = images.to(\"cpu\").detach()\n",
    "    masks = masks.to(\"cpu\").detach()\n",
    "    pred_masks = pred_masks.to(\"cpu\").detach()\n",
    "\n",
    "    for idx, (image, mask, pred_mask) in enumerate(zip(images, masks, pred_masks)):\n",
    "        if idx == n_samples:\n",
    "            break\n",
    "\n",
    "        if torch.is_tensor(image):\n",
    "            image = image.numpy()\n",
    "            image = image.transpose((1, 2, 0))\n",
    "\n",
    "        if torch.is_tensor(mask):\n",
    "            mask = mask.numpy()\n",
    "            mask = mask.squeeze(axis=0)\n",
    "\n",
    "        if torch.is_tensor(pred_mask):\n",
    "            pred_mask = pred_mask.numpy()\n",
    "            pred_mask = pred_mask.squeeze(axis=0)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image)\n",
    "        # plt.imshow(mask, alpha=0.6 * (mask > 0))\n",
    "        plt.imshow(pred_mask, alpha=0.6 * (pred_mask > 0))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def de_normalize(images, mean, std):\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).reshape((3, 1, 1)).to(DEVICE)\n",
    "        \n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).reshape((3, 1, 1)).to(DEVICE)\n",
    "    \n",
    "    images = images.to(DEVICE)\n",
    "    images = images * std + mean\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "def normalize(images, mean, std):\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).reshape((3, 1, 1)).to(DEVICE)\n",
    "        \n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).reshape((3, 1, 1)).to(DEVICE)\n",
    "    \n",
    "    images = images.to(DEVICE)\n",
    "    images = (images - mean) / std\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21707d4",
   "metadata": {
    "papermill": {
     "duration": 0.115574,
     "end_time": "2023-01-08T15:21:08.008197",
     "exception": false,
     "start_time": "2023-01-08T15:21:07.892623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implement Recognition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3695e69",
   "metadata": {
    "papermill": {
     "duration": 0.115541,
     "end_time": "2023-01-08T15:21:08.237866",
     "exception": false,
     "start_time": "2023-01-08T15:21:08.122325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8da5987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:08.496362Z",
     "iopub.status.busy": "2023-01-08T15:21:08.495773Z",
     "iopub.status.idle": "2023-01-08T15:21:13.097142Z",
     "shell.execute_reply": "2023-01-08T15:21:13.096183Z"
    },
    "papermill": {
     "duration": 4.738701,
     "end_time": "2023-01-08T15:21:13.099746",
     "exception": false,
     "start_time": "2023-01-08T15:21:08.361045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5701606b1dda4d34870450e15cc18710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from torchvision.models import resnet50, ResNet50_Weights\n",
    "#resnet50_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "resnet50_model = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d967216b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:13.331160Z",
     "iopub.status.busy": "2023-01-08T15:21:13.330748Z",
     "iopub.status.idle": "2023-01-08T15:21:13.336129Z",
     "shell.execute_reply": "2023-01-08T15:21:13.335073Z"
    },
    "papermill": {
     "duration": 0.123353,
     "end_time": "2023-01-08T15:21:13.338811",
     "exception": false,
     "start_time": "2023-01-08T15:21:13.215458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet50_feature_extractor = torch.nn.Sequential(*list(resnet50_model.children())[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd9f02",
   "metadata": {
    "papermill": {
     "duration": 0.111273,
     "end_time": "2023-01-08T15:21:13.561174",
     "exception": false,
     "start_time": "2023-01-08T15:21:13.449901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###  Load Finetuned DeepLabV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31cf8dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:13.788438Z",
     "iopub.status.busy": "2023-01-08T15:21:13.788041Z",
     "iopub.status.idle": "2023-01-08T15:21:13.794103Z",
     "shell.execute_reply": "2023-01-08T15:21:13.793166Z"
    },
    "papermill": {
     "duration": 0.12283,
     "end_time": "2023-01-08T15:21:13.796169",
     "exception": false,
     "start_time": "2023-01-08T15:21:13.673339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "import torchvision\n",
    "\n",
    "def get_model(out_channels=1, train_mode=False, pretrained=True):\n",
    "    \"\"\"\n",
    "    DeepLabv3 class with a custom head.\n",
    "    \"\"\"\n",
    "    # Load the pretrained model\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=pretrained)\n",
    "\n",
    "    # Replace the classifier module with a new head\n",
    "    model.classifier = DeepLabHead(2048, out_channels)\n",
    "\n",
    "    # Set model in appropriate mode\n",
    "    model.train() if train_mode else model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75337286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:14.026747Z",
     "iopub.status.busy": "2023-01-08T15:21:14.026036Z",
     "iopub.status.idle": "2023-01-08T15:21:26.097595Z",
     "shell.execute_reply": "2023-01-08T15:21:26.095084Z"
    },
    "papermill": {
     "duration": 12.191834,
     "end_time": "2023-01-08T15:21:26.101956",
     "exception": false,
     "start_time": "2023-01-08T15:21:13.910122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53754bcaf3794a1891cc9255e65d4034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/161M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "MODELS_DIR = \"./DeepLabV3_v1\"\n",
    "MODELS_DIR = \"/kaggle/input/seg-model/DeepLabV3_v1\"\n",
    "deeplabv3_model = get_model(out_channels=1, train_mode=False, pretrained=True)\n",
    "deeplabv3_model.load_state_dict(torch.load(MODELS_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f8e74",
   "metadata": {
    "papermill": {
     "duration": 0.370437,
     "end_time": "2023-01-08T15:21:26.699866",
     "exception": false,
     "start_time": "2023-01-08T15:21:26.329429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Local Binary Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1788d20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:27.172809Z",
     "iopub.status.busy": "2023-01-08T15:21:27.172437Z",
     "iopub.status.idle": "2023-01-08T15:21:27.970852Z",
     "shell.execute_reply": "2023-01-08T15:21:27.969874Z"
    },
    "papermill": {
     "duration": 0.937757,
     "end_time": "2023-01-08T15:21:27.973548",
     "exception": false,
     "start_time": "2023-01-08T15:21:27.035791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from skimage import feature\n",
    "from tqdm import tqdm\n",
    "from sympy import divisors\n",
    "\n",
    "\n",
    "def compute_concatenated_histogram(image, tile_size, num_bins):\n",
    "    \"\"\"\n",
    "    Function computes a concatenated histogram, by spliting the image\n",
    "    into tiles of size `tile_size` and computing a local histogram on each tile.\n",
    "    Histograms are normalized and concatenated together.\n",
    "\n",
    "    :param image: Input image\n",
    "    :param tile_size: Size of each tile (Width, Height)\n",
    "    :param num_bins: Number of bins used in computation of local histogram.\n",
    "    :return: Concatenated histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = image.shape\n",
    "    tile_height, tile_width = tile_size\n",
    "    fv = []\n",
    "\n",
    "    # Split image into tiles of size (tile_height x tile_width)\n",
    "    tiled_image = image.reshape(height // tile_height,\n",
    "                                tile_height,\n",
    "                                width // tile_width,\n",
    "                                tile_width)\n",
    "\n",
    "    tiled_image = tiled_image.swapaxes(1, 2)\n",
    "    tiled_image = tiled_image.reshape(tiled_image.shape[0] * tiled_image.shape[1], tile_height, tile_width)\n",
    "\n",
    "    # Compute histogram for each tile and concatenate them\n",
    "    for tile in tiled_image:\n",
    "        from scipy.sparse import csr_matrix\n",
    "        tile_hist, _ = np.histogram(tile, density=True, bins=num_bins, range=(0, num_bins))\n",
    "        fv.extend(tile_hist)\n",
    "\n",
    "    # Normalize the concatenated histogram\n",
    "    fv = np.array(fv)\n",
    "    # fv = fv / np.sum(fv)\n",
    "\n",
    "    return fv\n",
    "\n",
    "\n",
    "class ScikitLBP:\n",
    "    def __init__(self, num_points: int = 8, radius: int = 1, to_hist: bool = True, method: str = \"default\"):\n",
    "        self.num_points = num_points\n",
    "        self.radius = radius\n",
    "        self.to_hist = to_hist\n",
    "        self.method = method\n",
    "\n",
    "    def describe(self, images: npt.NDArray, *args, **kwargs) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Function computes a feature vector for each image.\n",
    "\n",
    "        :param images: A set of images.\n",
    "        :return: A set of feature vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        fvs = []\n",
    "\n",
    "        if self.to_hist:\n",
    "            image_size_divisors = list(divisors(images[0].shape[0], generator=True))[1: -1]\n",
    "            tile_width = kwargs.get(\"tile_width\", image_size_divisors[len(image_size_divisors) // 2])\n",
    "            # print(f\"tile width: {tile_width}\")\n",
    "            # print(f\"number of tiles: {(images[0].shape[0] / tile_width) ** 2}\")\n",
    "\n",
    "        # Extract features for each image\n",
    "        for image in images:\n",
    "            image_lbp = feature.local_binary_pattern(image, self.num_points, self.radius, method=self.method)\n",
    "\n",
    "            # Compute feature vector\n",
    "            if self.to_hist:\n",
    "                num_bins = int(image_lbp.max()) + 1\n",
    "                fv = compute_concatenated_histogram(image_lbp, tile_size=(tile_width, tile_width),\n",
    "                                                    num_bins=num_bins)\n",
    "            else:\n",
    "                fv = image_lbp.flatten()\n",
    "\n",
    "            # Store feature vector\n",
    "            fvs.append(fv)\n",
    "\n",
    "        fvs = np.array(fvs)\n",
    "        return fvs\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ScikitLBP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34bdac13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:28.201378Z",
     "iopub.status.busy": "2023-01-08T15:21:28.200782Z",
     "iopub.status.idle": "2023-01-08T15:21:28.210896Z",
     "shell.execute_reply": "2023-01-08T15:21:28.210033Z"
    },
    "papermill": {
     "duration": 0.125989,
     "end_time": "2023-01-08T15:21:28.212942",
     "exception": false,
     "start_time": "2023-01-08T15:21:28.086953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "OUT_IMAGE_HEIGHT, OUT_IMAGE_WIDTH = 128, 128\n",
    "\n",
    "def segment_ear(model, image, threshold):\n",
    "    image = image.unsqueeze(dim=0).to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Predict a segmentation mask and apply threshold\n",
    "    pred_mask = torch.sigmoid(model(image)[\"out\"])        \n",
    "    pred_mask[pred_mask >= threshold] = 1.0\n",
    "    pred_mask[pred_mask < threshold] = 0.0\n",
    "    \n",
    "    # check if empty mask\n",
    "    if not pred_mask.nonzero().sum().item():\n",
    "        return None\n",
    "\n",
    "    # Crop image using predicted seg. mask & rescale the image\n",
    "    cropped_image = crop_image(de_normalize(image, mean, std), pred_mask)\n",
    "    cropped_image = Image.fromarray(np.uint8(cropped_image * 255)).resize((OUT_IMAGE_HEIGHT, OUT_IMAGE_WIDTH))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def resnet_50_features(feature_extractor, image):\n",
    "    # Convert image to tensor\n",
    "    image_c = np.array(image).transpose((2, 0, 1))\n",
    "    image_c = torch.from_numpy(image_c)\n",
    "\n",
    "    # Normalize image\n",
    "    image_c = (image_c / 255.0).unsqueeze(dim=0)\n",
    "    image_c = normalize(image_c, mean, std)\n",
    "\n",
    "    # Move data and model on the device\n",
    "    feature_extractor = feature_extractor.to(DEVICE)\n",
    "    image_c = image_c.to(DEVICE)\n",
    "\n",
    "    feature_vect = feature_extractor(image_c).flatten()\n",
    "\n",
    "    return feature_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3b454",
   "metadata": {
    "papermill": {
     "duration": 0.111348,
     "end_time": "2023-01-08T15:21:28.436298",
     "exception": false,
     "start_time": "2023-01-08T15:21:28.324950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25413ca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:28.659838Z",
     "iopub.status.busy": "2023-01-08T15:21:28.659477Z",
     "iopub.status.idle": "2023-01-08T15:21:28.669868Z",
     "shell.execute_reply": "2023-01-08T15:21:28.668951Z"
    },
    "papermill": {
     "duration": 0.125421,
     "end_time": "2023-01-08T15:21:28.671728",
     "exception": false,
     "start_time": "2023-01-08T15:21:28.546307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def matching(\n",
    "    fv, db, dist_metric=\"euclidean\", n_closest=1\n",
    "):\n",
    "    # Fetch feature vectors from the database\n",
    "    db_labels = np.array(list(db.keys()))\n",
    "    db_fvs = np.array(list(db.values()))\n",
    "    \n",
    "    # Compute distance between fv and all samples in the database\n",
    "    # print(\"Computing distance matrix...\")\n",
    "    distances = distance.cdist(fv[np.newaxis, :], db_fvs, metric=dist_metric)[0]\n",
    "\n",
    "    # Sort distances\n",
    "    sorted_idxs = np.argsort(distances)\n",
    "    n_closest_samples = db_labels[sorted_idxs[0:n_closest]].copy()\n",
    "    n_closest_dists = distances[sorted_idxs[0:n_closest]].copy()\n",
    "    \n",
    "    del db_labels\n",
    "    del db_fvs\n",
    "    \n",
    "    return n_closest_samples, n_closest_dists\n",
    "\n",
    "\n",
    "def rec_pipeline(\n",
    "    test_image, seg_model, lbp_descriptor, resnet_model=None, \n",
    "    db=None, n_closest=1, dist_metric=\"euclidean\"\n",
    "):\n",
    "    \n",
    "    closest_samples, closest_dists = None, None\n",
    "    \n",
    "    # Segment ear from the image\n",
    "    cropped_image = segment_ear(seg_model, test_image, THRESHOLD)\n",
    "    #print(np.array(cropped_image).shape)\n",
    "    \n",
    "    if not cropped_image:\n",
    "        return closest_samples, closest_dists\n",
    "    \n",
    "    # Convert to gray scale\n",
    "    cropped_gray_image = np.array(cropped_image.convert(\"L\"))\n",
    "    \n",
    "    # Extract local binary patterns\n",
    "    fv = lbp_descriptor.describe(cropped_gray_image[np.newaxis, :, :], tile_width=TILE_WIDTH)[0]\n",
    "    \n",
    "    # ResNet50 features\n",
    "    if resnet_model:\n",
    "        resnet_fv = resnet_50_features(resnet_model, cropped_image).cpu()\n",
    "        fv = np.concatenate([fv, resnet_fv])\n",
    "    \n",
    "    # Find the closest samples in the database\n",
    "    closest_samples, closest_dists = matching(fv, db, dist_metric, n_closest)\n",
    "    \n",
    "    return closest_samples, closest_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05403ea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:28.897785Z",
     "iopub.status.busy": "2023-01-08T15:21:28.897417Z",
     "iopub.status.idle": "2023-01-08T15:21:28.904931Z",
     "shell.execute_reply": "2023-01-08T15:21:28.904042Z"
    },
    "papermill": {
     "duration": 0.123711,
     "end_time": "2023-01-08T15:21:28.907035",
     "exception": false,
     "start_time": "2023-01-08T15:21:28.783324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_database(database_dir):\n",
    "    database = {}\n",
    "\n",
    "    print(f\"Loading database: {database_dir}\")\n",
    "    for (dirpath, dirnames, filenames) in os.walk(database_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.pickle'):\n",
    "                full_path = os.sep.join([dirpath, filename])\n",
    "                identity = \"/\".join(full_path.strip(\".pickle\").split(\"/\")[-2:])\n",
    "                \n",
    "                with open(full_path, \"rb\") as handle:\n",
    "                    db_fv = pickle.load(handle)\n",
    "\n",
    "                database[identity] = db_fv\n",
    "                \n",
    "    return database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf9b29",
   "metadata": {
    "papermill": {
     "duration": 0.111019,
     "end_time": "2023-01-08T15:21:29.136072",
     "exception": false,
     "start_time": "2023-01-08T15:21:29.025053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pipeline Evaluation\n",
    "\n",
    "Evaluate the whole recognition pipeline, i.e use DeepLabV3 segmentations and feed cropped images to the matching stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14b65e",
   "metadata": {
    "papermill": {
     "duration": 0.112918,
     "end_time": "2023-01-08T15:21:29.359387",
     "exception": false,
     "start_time": "2023-01-08T15:21:29.246469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Make sure than non of the test samples are accidentally in the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "796b0662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:29.588175Z",
     "iopub.status.busy": "2023-01-08T15:21:29.587781Z",
     "iopub.status.idle": "2023-01-08T15:21:59.728131Z",
     "shell.execute_reply": "2023-01-08T15:21:59.727024Z"
    },
    "papermill": {
     "duration": 30.256826,
     "end_time": "2023-01-08T15:21:59.730671",
     "exception": false,
     "start_time": "2023-01-08T15:21:29.473845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database: /kaggle/input/ear-features1/lbp_features1\n"
     ]
    }
   ],
   "source": [
    "LBP_FEATURES_DIR = \"/kaggle/input/ear-features1/lbp_features1\"\n",
    "db = load_database(LBP_FEATURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3d984a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:21:59.957745Z",
     "iopub.status.busy": "2023-01-08T15:21:59.957373Z",
     "iopub.status.idle": "2023-01-08T15:21:59.964333Z",
     "shell.execute_reply": "2023-01-08T15:21:59.963399Z"
    },
    "papermill": {
     "duration": 0.12228,
     "end_time": "2023-01-08T15:21:59.966686",
     "exception": false,
     "start_time": "2023-01-08T15:21:59.844406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11201"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(db.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f564f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:22:00.202557Z",
     "iopub.status.busy": "2023-01-08T15:22:00.202158Z",
     "iopub.status.idle": "2023-01-08T15:23:43.057078Z",
     "shell.execute_reply": "2023-01-08T15:23:43.055943Z"
    },
    "papermill": {
     "duration": 102.981933,
     "end_time": "2023-01-08T15:23:43.059489",
     "exception": false,
     "start_time": "2023-01-08T15:22:00.077556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [01:42<00:00, 29.71it/s]\n"
     ]
    }
   ],
   "source": [
    "db_labels = np.array(list(db.keys()))\n",
    "\n",
    "for _, (_, test_label) in enumerate(tqdm(test_dataset)):\n",
    "    if test_label in list(db.keys()):\n",
    "        raise Expection(f\"test_label: {test_label} in the database !!!\")\n",
    "\n",
    "del db_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561010c",
   "metadata": {
    "papermill": {
     "duration": 0.154928,
     "end_time": "2023-01-08T15:23:43.371693",
     "exception": false,
     "start_time": "2023-01-08T15:23:43.216765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Random Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ed4c109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:23:43.670409Z",
     "iopub.status.busy": "2023-01-08T15:23:43.668532Z",
     "iopub.status.idle": "2023-01-08T15:23:43.674898Z",
     "shell.execute_reply": "2023-01-08T15:23:43.673955Z"
    },
    "papermill": {
     "duration": 0.158348,
     "end_time": "2023-01-08T15:23:43.676857",
     "exception": false,
     "start_time": "2023-01-08T15:23:43.518509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_model(identities, n_closest=1):\n",
    "    closest_samples = np.random.choice(list(identities.keys()), n_closest, replace=False)\n",
    "    return closest_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d61b2c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:23:43.970885Z",
     "iopub.status.busy": "2023-01-08T15:23:43.970520Z",
     "iopub.status.idle": "2023-01-08T15:24:56.455762Z",
     "shell.execute_reply": "2023-01-08T15:24:56.454665Z"
    },
    "papermill": {
     "duration": 72.638809,
     "end_time": "2023-01-08T15:24:56.459367",
     "exception": false,
     "start_time": "2023-01-08T15:23:43.820558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [01:12<00:00, 42.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1[%]: 0.19639934533551553\n",
      "rank 5[%]: 1.1456628477905073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rank_1_count, rank_5_count = 0, 0\n",
    "N_CLOSEST = 5\n",
    "\n",
    "for idx, (test_image, test_label) in enumerate(tqdm(test_dataset)):\n",
    "    closest_samples = random_model(db, n_closest=N_CLOSEST)\n",
    "    \n",
    "    closest_identities = [str(cs.split('/')[0]) for cs in closest_samples]\n",
    "    actual_identitiy = test_label.split(\"/\")[0]\n",
    "        \n",
    "    if actual_identitiy in closest_identities:\n",
    "        rank_5_count += 1\n",
    "            \n",
    "    if actual_identitiy == closest_identities[0]:\n",
    "        rank_1_count += 1\n",
    "\n",
    "rank_1 = 100 * (rank_1_count / len(test_dataset))\n",
    "rank_5 = 100 * (rank_5_count / len(test_dataset))\n",
    "\n",
    "print(f\"rank 1[%]: {rank_1}\")\n",
    "print(f\"rank 5[%]: {rank_5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae423a8a",
   "metadata": {
    "papermill": {
     "duration": 0.171813,
     "end_time": "2023-01-08T15:24:56.813440",
     "exception": false,
     "start_time": "2023-01-08T15:24:56.641627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Recognition using LBP feature vectors and concatenated LBP + ResNet50 feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b1f711d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:24:57.162412Z",
     "iopub.status.busy": "2023-01-08T15:24:57.162023Z",
     "iopub.status.idle": "2023-01-08T15:24:57.167184Z",
     "shell.execute_reply": "2023-01-08T15:24:57.166168Z"
    },
    "papermill": {
     "duration": 0.183255,
     "end_time": "2023-01-08T15:24:57.169189",
     "exception": false,
     "start_time": "2023-01-08T15:24:56.985934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.25\n",
    "NUM_POINTS = 8\n",
    "RADIUS = 1\n",
    "TO_HIST = True\n",
    "TILE_WIDTH = 16\n",
    "lbp_descriptor = ScikitLBP(num_points=NUM_POINTS, radius=RADIUS, to_hist=TO_HIST, method=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96cad111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:24:57.519061Z",
     "iopub.status.busy": "2023-01-08T15:24:57.516917Z",
     "iopub.status.idle": "2023-01-08T19:16:52.153868Z",
     "shell.execute_reply": "2023-01-08T19:16:52.152878Z"
    },
    "papermill": {
     "duration": 13915.048038,
     "end_time": "2023-01-08T19:16:52.389886",
     "exception": false,
     "start_time": "2023-01-08T15:24:57.341848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database: /kaggle/input/ear-features1/lbp_features1\n",
      "  mode distance metric              rank 1              rank 5\n",
      "0  LBP       euclidean  12.962356792144025  21.374795417348608\n",
      "1  LBP          cosine   14.63175122749591  23.404255319148938\n",
      "2  LBP       cityblock  19.410801963993453  29.885433715220948\n",
      "Loading database: /kaggle/input/ear-features1/lbp_resnet_features1\n",
      "           mode distance metric              rank 1              rank 5\n",
      "0  LBP_RESNET50       euclidean   12.76595744680851   23.20785597381342\n",
      "1  LBP_RESNET50          cosine  13.486088379705402  23.011456628477905\n",
      "2  LBP_RESNET50       cityblock   16.53027823240589   29.49263502454992\n"
     ]
    }
   ],
   "source": [
    "distance_metrics = [\"euclidean\", \"cosine\", \"cityblock\"]\n",
    "N_CLOSEST = 5\n",
    "LBP_FEATURES_DIR = \"/kaggle/input/ear-features1/lbp_features1\"\n",
    "LBP_RESNET_FEATURES_DIR = \"/kaggle/input/ear-features1/lbp_resnet_features1\"\n",
    "\n",
    "# Iterate over lbp features\n",
    "for mode in [\"LBP\", \"LBP_RESNET50\"]:\n",
    "    if mode == \"LBP\":\n",
    "        resnet_model = None\n",
    "        db = load_database(LBP_FEATURES_DIR) \n",
    "    else:\n",
    "        resnet_model = resnet50_feature_extractor\n",
    "        db = load_database(LBP_RESNET_FEATURES_DIR)\n",
    "        \n",
    "    run_stats = []\n",
    "    \n",
    "    # Try out all distance metrics\n",
    "    for dist_metric in distance_metrics:\n",
    "        rank_1_count, rank_5_count = 0, 0\n",
    "        \n",
    "        # Iterate over test samples and compute rank 1 and rank 5.\n",
    "        with torch.no_grad():\n",
    "            for idx, (test_image, test_label) in enumerate(test_dataset):\n",
    "                \n",
    "                # if idx == 5: break\n",
    "        \n",
    "                closest_samples, closest_dists = rec_pipeline(test_image, deeplabv3_model, lbp_descriptor, \n",
    "                                                      resnet_model=resnet_model,\n",
    "                                                      db=db, n_closest=N_CLOSEST, dist_metric=dist_metric)\n",
    "\n",
    "                if closest_samples is None:\n",
    "                    # print(\"Segmentation model failed !\")\n",
    "                    continue\n",
    "\n",
    "                closest_identities = [str(cs.split('/')[0]) for cs in closest_samples]\n",
    "                actual_identitiy = test_label.split(\"/\")[0]\n",
    "\n",
    "                if actual_identitiy in closest_identities:\n",
    "                    rank_5_count += 1\n",
    "\n",
    "                if actual_identitiy == closest_identities[0]:\n",
    "                    rank_1_count += 1\n",
    "\n",
    "        rank_1 = 100 * (rank_1_count / len(test_dataset))\n",
    "        rank_5 = 100 * (rank_5_count / len(test_dataset))\n",
    "        run_stats.append([mode, dist_metric, rank_1, rank_5])\n",
    "        \n",
    "    # Create a pandas dataframe and save it\n",
    "    run_stats_df = pd.DataFrame(np.array(run_stats), columns=[\"mode\", \"distance metric\", \"rank 1\", \"rank 5\"])\n",
    "    print(run_stats_df)\n",
    "    run_stats_df.to_csv(f\"./{mode}_run_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635754ad",
   "metadata": {
    "papermill": {
     "duration": 0.17879,
     "end_time": "2023-01-08T19:16:52.743402",
     "exception": false,
     "start_time": "2023-01-08T19:16:52.564612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Recognition Evaluation\n",
    "\n",
    "Evaluate only recognition part of the pipeline, i.e use ground truth segmentations and feed cropped images to the matching stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4cb10b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:16:53.098379Z",
     "iopub.status.busy": "2023-01-08T19:16:53.097786Z",
     "iopub.status.idle": "2023-01-08T19:16:53.126340Z",
     "shell.execute_reply": "2023-01-08T19:16:53.125077Z"
    },
    "papermill": {
     "duration": 0.209073,
     "end_time": "2023-01-08T19:16:53.128326",
     "exception": false,
     "start_time": "2023-01-08T19:16:52.919253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3055\n"
     ]
    }
   ],
   "source": [
    "class EarDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, annotations_file, image_dir=\"./images\", mask_dir=\"./masks\", transform=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.annotations = pd.read_csv(annotations_file, delimiter=\"\\t\", dtype=\"str\")\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Fetch image and mask name\n",
    "        folder_name, file_name = np.array(self.annotations.iloc[idx, :2])\n",
    "        image_name = os.path.join(self.image_dir, f\"{folder_name}/{file_name}.png\")\n",
    "        mask_name = os.path.join(self.mask_dir, f\"{folder_name}/{file_name}.png\")\n",
    "\n",
    "        # Read image and mask & convert to numpy array\n",
    "        image = np.array(PIL.Image.open(image_name))\n",
    "        mask = np.array(PIL.Image.open(mask_name).convert(\"L\"))\n",
    "        mask = mask[:, :, np.newaxis]\n",
    "        mask[mask == 255.0] = 1.0\n",
    "        \n",
    "        label = f\"{folder_name}/{file_name}\"\n",
    "\n",
    "        if self.transform:\n",
    "            augmentation = self.transform(image=image, mask=mask)\n",
    "            image, mask = augmentation[\"image\"], augmentation[\"mask\"] \n",
    "\n",
    "        return image, mask, label\n",
    "    \n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 300, 300\n",
    "preprocess_transform = albumentations.Compose([\n",
    "    albumentations.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
    "    albumentations.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "test_dataset = EarDataset(annotations_file=f\"/kaggle/input/tempdata/annotations_test.csv\",\n",
    "                                image_dir=f\"{ROOT_PATH}/images\",\n",
    "                                mask_dir=f\"{ROOT_PATH}/masks\",\n",
    "                                transform=preprocess_transform)\n",
    "\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "156a80f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:16:53.481036Z",
     "iopub.status.busy": "2023-01-08T19:16:53.480383Z",
     "iopub.status.idle": "2023-01-08T19:16:53.519818Z",
     "shell.execute_reply": "2023-01-08T19:16:53.518796Z"
    },
    "papermill": {
     "duration": 0.21919,
     "end_time": "2023-01-08T19:16:53.522310",
     "exception": false,
     "start_time": "2023-01-08T19:16:53.303120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 300, 300]), torch.Size([1, 300, 300]), '001/03')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, mask, label = test_dataset[0]\n",
    "image.shape, mask.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56eb1159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:16:53.873400Z",
     "iopub.status.busy": "2023-01-08T19:16:53.872331Z",
     "iopub.status.idle": "2023-01-08T19:16:53.878836Z",
     "shell.execute_reply": "2023-01-08T19:16:53.877970Z"
    },
    "papermill": {
     "duration": 0.185324,
     "end_time": "2023-01-08T19:16:53.880941",
     "exception": false,
     "start_time": "2023-01-08T19:16:53.695617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_ear_ground_truth(image, gt_mask):\n",
    "    cropped_image = crop_image(image, gt_mask)\n",
    "    cropped_image = Image.fromarray(np.uint8(cropped_image * 255.0)).resize((OUT_IMAGE_HEIGHT, OUT_IMAGE_WIDTH))\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df44e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:16:54.231265Z",
     "iopub.status.busy": "2023-01-08T19:16:54.230679Z",
     "iopub.status.idle": "2023-01-08T19:16:54.238287Z",
     "shell.execute_reply": "2023-01-08T19:16:54.237350Z"
    },
    "papermill": {
     "duration": 0.187211,
     "end_time": "2023-01-08T19:16:54.240285",
     "exception": false,
     "start_time": "2023-01-08T19:16:54.053074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def rec_pipeline_gt(\n",
    "    test_image, test_mask, lbp_descriptor, resnet_model=None, \n",
    "    db=None, n_closest=1, dist_metric=\"euclidean\"\n",
    "):\n",
    "    closest_samples, closest_dists = None, None\n",
    "    \n",
    "    #########################################################################################\n",
    "    cropped_image = segment_ear_ground_truth(test_image.unsqueeze(0), test_mask.unsqueeze(0))\n",
    "    #print(np.array(cropped_image).shape)\n",
    "    #########################################################################################\n",
    "    \n",
    "    # Convert to gray scale\n",
    "    cropped_gray_image = np.array(cropped_image.convert(\"L\"))\n",
    "    \n",
    "    # Extract local binary patterns\n",
    "    fv = lbp_descriptor.describe(cropped_gray_image[np.newaxis, :, :], tile_width=TILE_WIDTH)[0]\n",
    "    \n",
    "    # ResNet50 features\n",
    "    if resnet_model:\n",
    "        resnet_fv = resnet_50_features(resnet_model, cropped_image).cpu()\n",
    "        fv = np.concatenate([fv, resnet_fv])\n",
    "    \n",
    "    # Find the closest samples in the database\n",
    "    closest_samples, closest_dists = matching(fv, db, dist_metric, n_closest)\n",
    "    \n",
    "    return closest_samples, closest_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c72dede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:16:54.588824Z",
     "iopub.status.busy": "2023-01-08T19:16:54.588277Z",
     "iopub.status.idle": "2023-01-08T23:01:57.632235Z",
     "shell.execute_reply": "2023-01-08T23:01:57.631181Z"
    },
    "papermill": {
     "duration": 13503.402683,
     "end_time": "2023-01-08T23:01:57.813924",
     "exception": false,
     "start_time": "2023-01-08T19:16:54.411241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database: /kaggle/input/ear-features1/lbp_features_gt\n",
      "     mode distance metric              rank 1              rank 5\n",
      "0  LBP_gt       euclidean    15.3518821603928   26.21931260229133\n",
      "1  LBP_gt          cosine  17.315875613747956  29.099836333878887\n",
      "2  LBP_gt       cityblock   23.04418985270049   36.46481178396072\n",
      "Loading database: /kaggle/input/ear-features1/lbp_resnet_features_gt\n",
      "              mode distance metric              rank 1              rank 5\n",
      "0  LBP_RESNET50_gt       euclidean  16.104746317512276  27.135842880523732\n",
      "1  LBP_RESNET50_gt          cosine  16.661211129296234   27.95417348608838\n",
      "2  LBP_RESNET50_gt       cityblock  20.621931260229132   34.46808510638298\n"
     ]
    }
   ],
   "source": [
    "distance_metrics = [\"euclidean\", \"cosine\", \"cityblock\"]\n",
    "N_CLOSEST = 5\n",
    "LBP_FEATURES_DIR = \"/kaggle/input/ear-features1/lbp_features_gt\"\n",
    "LBP_RESNET_FEATURES_DIR = \"/kaggle/input/ear-features1/lbp_resnet_features_gt\"\n",
    "\n",
    "# Iterate over lbp features\n",
    "for mode in [\"LBP_gt\", \"LBP_RESNET50_gt\"]:\n",
    "    if mode == \"LBP_gt\":\n",
    "        resnet_model = None\n",
    "        db = load_database(LBP_FEATURES_DIR) \n",
    "    else:\n",
    "        resnet_model = resnet50_feature_extractor\n",
    "        db = load_database(LBP_RESNET_FEATURES_DIR)\n",
    "        \n",
    "    run_stats = []\n",
    "    \n",
    "    # Try out all distance metrics\n",
    "    for dist_metric in distance_metrics:\n",
    "        rank_1_count, rank_5_count = 0, 0\n",
    "        \n",
    "        # Iterate over test samples and compute rank 1 and rank 5.\n",
    "        with torch.no_grad():\n",
    "            for idx, (test_image, test_mask, test_label) in enumerate(test_dataset):\n",
    "                \n",
    "                # if idx == 5: break\n",
    "        \n",
    "                closest_samples, closest_dists = rec_pipeline_gt(test_image, test_mask, lbp_descriptor, \n",
    "                                                      resnet_model=resnet_model,\n",
    "                                                      db=db, n_closest=N_CLOSEST, dist_metric=dist_metric)\n",
    "\n",
    "\n",
    "                closest_identities = [str(cs.split('/')[0]) for cs in closest_samples]\n",
    "                actual_identitiy = test_label.split(\"/\")[0]\n",
    "\n",
    "                if actual_identitiy in closest_identities:\n",
    "                    rank_5_count += 1\n",
    "\n",
    "                if actual_identitiy == closest_identities[0]:\n",
    "                    rank_1_count += 1\n",
    "\n",
    "        rank_1 = 100 * (rank_1_count / len(test_dataset))\n",
    "        rank_5 = 100 * (rank_5_count / len(test_dataset))\n",
    "        run_stats.append([mode, dist_metric, rank_1, rank_5])\n",
    "        \n",
    "    # Create a pandas dataframe and save it\n",
    "    run_stats_df = pd.DataFrame(np.array(run_stats), columns=[\"mode\", \"distance metric\", \"rank 1\", \"rank 5\"])\n",
    "    print(run_stats_df)\n",
    "    run_stats_df.to_csv(f\"./{mode}_run_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f432842",
   "metadata": {
    "papermill": {
     "duration": 0.173631,
     "end_time": "2023-01-08T23:01:58.160822",
     "exception": false,
     "start_time": "2023-01-08T23:01:57.987191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27687.999613,
   "end_time": "2023-01-08T23:02:00.594916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-08T15:20:32.595303",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06c66d0fcfa542ff87813b99e8a14d60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0ab26bfeefe3440ab60ba91b0b212ce5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_af7c21b34a804b85a48c8b08417e5479",
       "max": 102530333,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eca2dec5400f4640a14b3b505c95fea7",
       "value": 102530333
      }
     },
     "283b49adc96e4015aa4734010b082e13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_43a929db015b407d9e18d63f9c1b8a3c",
       "max": 168312152,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_06c66d0fcfa542ff87813b99e8a14d60",
       "value": 168312152
      }
     },
     "43a929db015b407d9e18d63f9c1b8a3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fa9465154b74284b76526bda2f012bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53754bcaf3794a1891cc9255e65d4034": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d211e0e9d02040d89657bc905394e84d",
        "IPY_MODEL_283b49adc96e4015aa4734010b082e13",
        "IPY_MODEL_9d3e184741cc48e9b9936f60d752c91f"
       ],
       "layout": "IPY_MODEL_4fa9465154b74284b76526bda2f012bf"
      }
     },
     "5701606b1dda4d34870450e15cc18710": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e3358bf3dc2b4b5cb732301835e6ef24",
        "IPY_MODEL_0ab26bfeefe3440ab60ba91b0b212ce5",
        "IPY_MODEL_d5b8c8ba327d47f498f4aeffb625332f"
       ],
       "layout": "IPY_MODEL_ac036691a2b747e886291ce685c88a6f"
      }
     },
     "5b7e2237dcce4d19852856bbb0918586": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5bdb47992a9f4f6ab2177d15c97fc187": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "67b5404170a742168b7deeb343b7413f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9625278fd6404b04b9ac3bb80c5d99be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "971732da64e7459cb2e89db80ea623e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9aaeb2d37e6749039f6e9de9ba60d6b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9d3e184741cc48e9b9936f60d752c91f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e53556f1b5c64ebd826f172173909c6d",
       "placeholder": "​",
       "style": "IPY_MODEL_fe09f52311884504812b480cfee10c4f",
       "value": " 161M/161M [00:05&lt;00:00, 32.6MB/s]"
      }
     },
     "ac036691a2b747e886291ce685c88a6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af7c21b34a804b85a48c8b08417e5479": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d211e0e9d02040d89657bc905394e84d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5bdb47992a9f4f6ab2177d15c97fc187",
       "placeholder": "​",
       "style": "IPY_MODEL_5b7e2237dcce4d19852856bbb0918586",
       "value": "100%"
      }
     },
     "d5b8c8ba327d47f498f4aeffb625332f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_971732da64e7459cb2e89db80ea623e2",
       "placeholder": "​",
       "style": "IPY_MODEL_9625278fd6404b04b9ac3bb80c5d99be",
       "value": " 97.8M/97.8M [00:03&lt;00:00, 32.7MB/s]"
      }
     },
     "e3358bf3dc2b4b5cb732301835e6ef24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_67b5404170a742168b7deeb343b7413f",
       "placeholder": "​",
       "style": "IPY_MODEL_9aaeb2d37e6749039f6e9de9ba60d6b9",
       "value": "100%"
      }
     },
     "e53556f1b5c64ebd826f172173909c6d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eca2dec5400f4640a14b3b505c95fea7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fe09f52311884504812b480cfee10c4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
