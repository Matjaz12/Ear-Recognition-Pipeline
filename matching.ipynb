{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe16f26d",
   "metadata": {
    "papermill": {
     "duration": 0.013008,
     "end_time": "2023-01-07T22:51:39.095857",
     "exception": false,
     "start_time": "2023-01-07T22:51:39.082849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Matching\n",
    "\n",
    "In this notebook we implement and evaluate (compute rank 1 and rank 5 accuracy) the matching stage of a recognition pipeline. Database of registered individuals stores features extracted using notebook [1]. We iterate over all non-registered samples, apply the fine-tuned segmentation model [2], crop the ear from the image, extract features using MLBP and ResNet50 (pretrained on ImageNet) and finally find the closests sample in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d08eb",
   "metadata": {
    "papermill": {
     "duration": 0.010491,
     "end_time": "2023-01-07T22:51:39.117508",
     "exception": false,
     "start_time": "2023-01-07T22:51:39.107017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**References**\n",
    "\n",
    "[1] https://github.com/Matjaz12/Ear-Recognition-Pipeline/blob/main/feature_extraction.ipynb\n",
    "\n",
    "[2] https://github.com/Matjaz12/Ear-Recognition-Pipeline/blob/main/segmentation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6880af",
   "metadata": {
    "papermill": {
     "duration": 0.011627,
     "end_time": "2023-01-07T22:51:39.140425",
     "exception": false,
     "start_time": "2023-01-07T22:51:39.128798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import dependencies, mount google drive and unzip data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483e1040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:39.157424Z",
     "iopub.status.busy": "2023-01-07T22:51:39.156580Z",
     "iopub.status.idle": "2023-01-07T22:51:42.277532Z",
     "shell.execute_reply": "2023-01-07T22:51:42.276562Z"
    },
    "papermill": {
     "duration": 3.132382,
     "end_time": "2023-01-07T22:51:42.280020",
     "exception": false,
     "start_time": "2023-01-07T22:51:39.147638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skimage import io, transform\n",
    "import PIL\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, utils\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad929e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:42.297677Z",
     "iopub.status.busy": "2023-01-07T22:51:42.295827Z",
     "iopub.status.idle": "2023-01-07T22:51:42.301071Z",
     "shell.execute_reply": "2023-01-07T22:51:42.300158Z"
    },
    "papermill": {
     "duration": 0.015541,
     "end_time": "2023-01-07T22:51:42.303159",
     "exception": false,
     "start_time": "2023-01-07T22:51:42.287618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#FOLDERNAME = \"IBB5\"\n",
    "#assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "#import sys\n",
    "#DATA_PATH = '/content/drive/My Drive/{}'.format(FOLDERNAME)\n",
    "#print(DATA_PATH)\n",
    "#sys.path.append(DATA_PATH)\n",
    "\n",
    "#ROOT_PATH = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2d0373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:42.320744Z",
     "iopub.status.busy": "2023-01-07T22:51:42.319947Z",
     "iopub.status.idle": "2023-01-07T22:51:42.324491Z",
     "shell.execute_reply": "2023-01-07T22:51:42.323694Z"
    },
    "papermill": {
     "duration": 0.014658,
     "end_time": "2023-01-07T22:51:42.326522",
     "exception": false,
     "start_time": "2023-01-07T22:51:42.311864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip \"/content/drive/My Drive/IBB5/data.zip\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912dcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "ROOT_PATH = \"/kaggle/input/eardataset/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f29846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:57.453344Z",
     "iopub.status.busy": "2023-01-07T22:51:57.453007Z",
     "iopub.status.idle": "2023-01-07T22:51:57.461044Z",
     "shell.execute_reply": "2023-01-07T22:51:57.460090Z"
    },
    "papermill": {
     "duration": 0.073375,
     "end_time": "2023-01-07T22:51:57.462982",
     "exception": false,
     "start_time": "2023-01-07T22:51:57.389607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, annotations_file, image_dir=\"./images\", transform=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.annotations = pd.read_csv(annotations_file, delimiter=\"\\t\", dtype=\"str\")\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Fetch image\n",
    "        folder_name, file_name = np.array(self.annotations.iloc[idx, :2])\n",
    "        image_name = os.path.join(self.image_dir, f\"{folder_name}/{file_name}.png\")\n",
    "        label = f\"{folder_name}/{file_name}\"\n",
    "\n",
    "        # Read the image and covert to numpy array\n",
    "        image = np.array(PIL.Image.open(image_name))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentation = self.transform(image=image)\n",
    "            image = augmentation[\"image\"]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433db881",
   "metadata": {
    "papermill": {
     "duration": 0.060339,
     "end_time": "2023-01-07T22:51:57.586448",
     "exception": false,
     "start_time": "2023-01-07T22:51:57.526109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Copy over the hyper-parameters used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0ebce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:57.716339Z",
     "iopub.status.busy": "2023-01-07T22:51:57.715969Z",
     "iopub.status.idle": "2023-01-07T22:51:57.783800Z",
     "shell.execute_reply": "2023-01-07T22:51:57.782924Z"
    },
    "papermill": {
     "duration": 0.139697,
     "end_time": "2023-01-07T22:51:57.786061",
     "exception": false,
     "start_time": "2023-01-07T22:51:57.646364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0 # 2\n",
    "IMAGE_HEIGHT = 300 \n",
    "IMAGE_WIDTH = 300 \n",
    "PIN_MEMORY = True\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199b7da",
   "metadata": {
    "papermill": {
     "duration": 0.060401,
     "end_time": "2023-01-07T22:51:57.911069",
     "exception": false,
     "start_time": "2023-01-07T22:51:57.850668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Load the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791756f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:58.036597Z",
     "iopub.status.busy": "2023-01-07T22:51:58.034924Z",
     "iopub.status.idle": "2023-01-07T22:51:59.053122Z",
     "shell.execute_reply": "2023-01-07T22:51:59.051456Z"
    },
    "papermill": {
     "duration": 1.083709,
     "end_time": "2023-01-07T22:51:59.055983",
     "exception": false,
     "start_time": "2023-01-07T22:51:57.972274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset): 3055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 300, 300]), '001/03')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations\n",
    "\n",
    "preprocess_transform = albumentations.Compose([\n",
    "    albumentations.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
    "    albumentations.Normalize(mean=mean, std=std, max_pixel_value=255.0),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "test_dataset = EarClassificationDataset(annotations_file=f\"/kaggle/input/tempdata/annotations_test.csv\",\n",
    "                                image_dir=f\"{ROOT_PATH}/images\",\n",
    "                                transform=preprocess_transform)\n",
    "\n",
    "print(f\"len(test_dataset): {len(test_dataset)}\")\n",
    "image, label = test_dataset[0]\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf814e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:59.183437Z",
     "iopub.status.busy": "2023-01-07T22:51:59.183076Z",
     "iopub.status.idle": "2023-01-07T22:51:59.203400Z",
     "shell.execute_reply": "2023-01-07T22:51:59.202486Z"
    },
    "papermill": {
     "duration": 0.086112,
     "end_time": "2023-01-07T22:51:59.205479",
     "exception": false,
     "start_time": "2023-01-07T22:51:59.119367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_image(image, pred_mask):\n",
    "    image = image.to(\"cpu\").detach()\n",
    "    pred_mask = pred_mask.to(\"cpu\").detach()\n",
    "\n",
    "    image = image[0].numpy()\n",
    "    image = image.transpose((1, 2, 0))\n",
    "\n",
    "    pred_mask = pred_mask[0].numpy()\n",
    "    pred_mask = pred_mask.squeeze(axis=0)\n",
    "\n",
    "    # Compute masked image\n",
    "    pred_mask = np.stack((pred_mask, ) * 3, axis=-1)\n",
    "    masked_image = image * pred_mask\n",
    "\n",
    "    # Compute cropped image\n",
    "    nonzero_rows, nonzero_cols = np.nonzero(masked_image.mean(axis=2))\n",
    "    seg_y_start, seg_y_end = nonzero_rows.min(), nonzero_rows.max() + 1\n",
    "    seg_x_start, seg_x_end = nonzero_cols.min(), nonzero_cols.max() + 1\n",
    "\n",
    "    cropped_image = masked_image[seg_y_start:seg_y_end, seg_x_start:seg_x_end, :]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def plot_image_and_cropped_ear(image, pred_mask, cropped_image):\n",
    "    image = image[0].to(\"cpu\").detach()\n",
    "    pred_mask = pred_mask[0].to(\"cpu\").detach()\n",
    "\n",
    "    if torch.is_tensor(image):\n",
    "        image = image.numpy()\n",
    "        image = image.transpose((1, 2, 0))\n",
    "\n",
    "    if torch.is_tensor(pred_mask):\n",
    "        pred_mask = pred_mask.numpy()\n",
    "        pred_mask = pred_mask.squeeze(axis=0)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(image)\n",
    "    ax1.imshow(pred_mask, alpha=0.6 * (pred_mask > 0))\n",
    "    ax2.imshow(cropped_image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions(images, masks, pred_masks, n_samples=3):\n",
    "    images = images.to(\"cpu\").detach()\n",
    "    masks = masks.to(\"cpu\").detach()\n",
    "    pred_masks = pred_masks.to(\"cpu\").detach()\n",
    "\n",
    "    for idx, (image, mask, pred_mask) in enumerate(zip(images, masks, pred_masks)):\n",
    "        if idx == n_samples:\n",
    "            break\n",
    "\n",
    "        if torch.is_tensor(image):\n",
    "            image = image.numpy()\n",
    "            image = image.transpose((1, 2, 0))\n",
    "\n",
    "        if torch.is_tensor(mask):\n",
    "            mask = mask.numpy()\n",
    "            mask = mask.squeeze(axis=0)\n",
    "\n",
    "        if torch.is_tensor(pred_mask):\n",
    "            pred_mask = pred_mask.numpy()\n",
    "            pred_mask = pred_mask.squeeze(axis=0)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image)\n",
    "        # plt.imshow(mask, alpha=0.6 * (mask > 0))\n",
    "        plt.imshow(pred_mask, alpha=0.6 * (pred_mask > 0))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def de_normalize(images, mean, std):\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).reshape((3, 1, 1)).to(DEVICE)\n",
    "        \n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).reshape((3, 1, 1)).to(DEVICE)\n",
    "    \n",
    "    images = images.to(DEVICE)\n",
    "    images = images * std + mean\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "def normalize(images, mean, std):\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).reshape((3, 1, 1)).to(DEVICE)\n",
    "        \n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).reshape((3, 1, 1)).to(DEVICE)\n",
    "    \n",
    "    images = images.to(DEVICE)\n",
    "    images = (images - mean) / std\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b3629",
   "metadata": {
    "papermill": {
     "duration": 0.060569,
     "end_time": "2023-01-07T22:51:59.327475",
     "exception": false,
     "start_time": "2023-01-07T22:51:59.266906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88293e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:51:59.451857Z",
     "iopub.status.busy": "2023-01-07T22:51:59.451512Z",
     "iopub.status.idle": "2023-01-07T22:52:04.090902Z",
     "shell.execute_reply": "2023-01-07T22:52:04.089862Z"
    },
    "papermill": {
     "duration": 4.704354,
     "end_time": "2023-01-07T22:52:04.093523",
     "exception": false,
     "start_time": "2023-01-07T22:51:59.389169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0949b08fdf2241938647433614fc064e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from torchvision.models import resnet50, ResNet50_Weights\n",
    "#resnet50_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "resnet50_model = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e5a7c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:04.220880Z",
     "iopub.status.busy": "2023-01-07T22:52:04.220180Z",
     "iopub.status.idle": "2023-01-07T22:52:04.225919Z",
     "shell.execute_reply": "2023-01-07T22:52:04.224971Z"
    },
    "papermill": {
     "duration": 0.070454,
     "end_time": "2023-01-07T22:52:04.228018",
     "exception": false,
     "start_time": "2023-01-07T22:52:04.157564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet50_feature_extractor = torch.nn.Sequential(*list(resnet50_model.children())[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5aab2",
   "metadata": {
    "papermill": {
     "duration": 0.05935,
     "end_time": "2023-01-07T22:52:04.347150",
     "exception": false,
     "start_time": "2023-01-07T22:52:04.287800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  Load Finetuned DeepLabV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c16399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:04.471526Z",
     "iopub.status.busy": "2023-01-07T22:52:04.470802Z",
     "iopub.status.idle": "2023-01-07T22:52:04.477552Z",
     "shell.execute_reply": "2023-01-07T22:52:04.476514Z"
    },
    "papermill": {
     "duration": 0.07155,
     "end_time": "2023-01-07T22:52:04.479735",
     "exception": false,
     "start_time": "2023-01-07T22:52:04.408185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "import torchvision\n",
    "\n",
    "def get_model(out_channels=1, train_mode=False, pretrained=True):\n",
    "    \"\"\"\n",
    "    DeepLabv3 class with a custom head.\n",
    "    \"\"\"\n",
    "    # Load the pretrained model\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=pretrained)\n",
    "\n",
    "    # Replace the classifier module with a new head\n",
    "    model.classifier = DeepLabHead(2048, out_channels)\n",
    "\n",
    "    # Set model in appropriate mode\n",
    "    model.train() if train_mode else model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da90173b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:04.601537Z",
     "iopub.status.busy": "2023-01-07T22:52:04.599731Z",
     "iopub.status.idle": "2023-01-07T22:52:16.517912Z",
     "shell.execute_reply": "2023-01-07T22:52:16.516943Z"
    },
    "papermill": {
     "duration": 11.981142,
     "end_time": "2023-01-07T22:52:16.520397",
     "exception": false,
     "start_time": "2023-01-07T22:52:04.539255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78718b118f446e5b309bb1437d341df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/161M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "MODELS_DIR = \"./DeepLabV3_v1\"\n",
    "MODELS_DIR = \"/kaggle/input/seg-model/DeepLabV3_v1\"\n",
    "deeplabv3_model = get_model(out_channels=1, train_mode=False, pretrained=True)\n",
    "deeplabv3_model.load_state_dict(torch.load(MODELS_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210c5f4",
   "metadata": {
    "papermill": {
     "duration": 0.067892,
     "end_time": "2023-01-07T22:52:16.655042",
     "exception": false,
     "start_time": "2023-01-07T22:52:16.587150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Local Binary Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e32f871b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:16.791658Z",
     "iopub.status.busy": "2023-01-07T22:52:16.791165Z",
     "iopub.status.idle": "2023-01-07T22:52:17.549793Z",
     "shell.execute_reply": "2023-01-07T22:52:17.548682Z"
    },
    "papermill": {
     "duration": 0.829422,
     "end_time": "2023-01-07T22:52:17.552402",
     "exception": false,
     "start_time": "2023-01-07T22:52:16.722980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from skimage import feature\n",
    "from tqdm import tqdm\n",
    "from sympy import divisors\n",
    "\n",
    "\n",
    "def compute_concatenated_histogram(image, tile_size, num_bins):\n",
    "    \"\"\"\n",
    "    Function computes a concatenated histogram, by spliting the image\n",
    "    into tiles of size `tile_size` and computing a local histogram on each tile.\n",
    "    Histograms are normalized and concatenated together.\n",
    "\n",
    "    :param image: Input image\n",
    "    :param tile_size: Size of each tile (Width, Height)\n",
    "    :param num_bins: Number of bins used in computation of local histogram.\n",
    "    :return: Concatenated histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = image.shape\n",
    "    tile_height, tile_width = tile_size\n",
    "    fv = []\n",
    "\n",
    "    # Split image into tiles of size (tile_height x tile_width)\n",
    "    tiled_image = image.reshape(height // tile_height,\n",
    "                                tile_height,\n",
    "                                width // tile_width,\n",
    "                                tile_width)\n",
    "\n",
    "    tiled_image = tiled_image.swapaxes(1, 2)\n",
    "    tiled_image = tiled_image.reshape(tiled_image.shape[0] * tiled_image.shape[1], tile_height, tile_width)\n",
    "\n",
    "    # Compute histogram for each tile and concatenate them\n",
    "    for tile in tiled_image:\n",
    "        from scipy.sparse import csr_matrix\n",
    "        tile_hist, _ = np.histogram(tile, density=True, bins=num_bins, range=(0, num_bins))\n",
    "        fv.extend(tile_hist)\n",
    "\n",
    "    # Normalize the concatenated histogram\n",
    "    fv = np.array(fv)\n",
    "    # fv = fv / np.sum(fv)\n",
    "\n",
    "    return fv\n",
    "\n",
    "\n",
    "class ScikitLBP:\n",
    "    def __init__(self, num_points: int = 8, radius: int = 1, to_hist: bool = True, method: str = \"default\"):\n",
    "        self.num_points = num_points\n",
    "        self.radius = radius\n",
    "        self.to_hist = to_hist\n",
    "        self.method = method\n",
    "\n",
    "    def describe(self, images: npt.NDArray, *args, **kwargs) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Function computes a feature vector for each image.\n",
    "\n",
    "        :param images: A set of images.\n",
    "        :return: A set of feature vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        fvs = []\n",
    "\n",
    "        if self.to_hist:\n",
    "            image_size_divisors = list(divisors(images[0].shape[0], generator=True))[1: -1]\n",
    "            tile_width = kwargs.get(\"tile_width\", image_size_divisors[len(image_size_divisors) // 2])\n",
    "            # print(f\"tile width: {tile_width}\")\n",
    "            # print(f\"number of tiles: {(images[0].shape[0] / tile_width) ** 2}\")\n",
    "\n",
    "        # Extract features for each image\n",
    "        for image in images:\n",
    "            image_lbp = feature.local_binary_pattern(image, self.num_points, self.radius, method=self.method)\n",
    "\n",
    "            # Compute feature vector\n",
    "            if self.to_hist:\n",
    "                num_bins = int(image_lbp.max()) + 1\n",
    "                fv = compute_concatenated_histogram(image_lbp, tile_size=(tile_width, tile_width),\n",
    "                                                    num_bins=num_bins)\n",
    "            else:\n",
    "                fv = image_lbp.flatten()\n",
    "\n",
    "            # Store feature vector\n",
    "            fvs.append(fv)\n",
    "\n",
    "        fvs = np.array(fvs)\n",
    "        return fvs\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ScikitLBP\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd02106",
   "metadata": {
    "papermill": {
     "duration": 0.062456,
     "end_time": "2023-01-07T22:52:17.679633",
     "exception": false,
     "start_time": "2023-01-07T22:52:17.617177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Recognition Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04c2add5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:17.801974Z",
     "iopub.status.busy": "2023-01-07T22:52:17.801441Z",
     "iopub.status.idle": "2023-01-07T22:52:17.811200Z",
     "shell.execute_reply": "2023-01-07T22:52:17.810323Z"
    },
    "papermill": {
     "duration": 0.074435,
     "end_time": "2023-01-07T22:52:17.813217",
     "exception": false,
     "start_time": "2023-01-07T22:52:17.738782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "OUT_IMAGE_HEIGHT, OUT_IMAGE_WIDTH = 128, 128\n",
    "\n",
    "def segment_ear(model, image, threshold):\n",
    "    image = image.unsqueeze(dim=0).to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Predict a segmentation mask and apply threshold\n",
    "    pred_mask = torch.sigmoid(model(image)[\"out\"])        \n",
    "    pred_mask[pred_mask >= threshold] = 1.0\n",
    "    pred_mask[pred_mask < threshold] = 0.0\n",
    "    \n",
    "    # check if empty mask\n",
    "    if not pred_mask.nonzero().sum().item():\n",
    "        return None\n",
    "\n",
    "    # Crop image using predicted seg. mask & rescale the image\n",
    "    cropped_image = crop_image(de_normalize(image, mean, std), pred_mask)\n",
    "    cropped_image = Image.fromarray(np.uint8(cropped_image * 255)).resize((OUT_IMAGE_HEIGHT, OUT_IMAGE_WIDTH))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def resnet_50_features(feature_extractor, image):\n",
    "    # Convert image to tensor\n",
    "    image_c = np.array(image).transpose((2, 0, 1))\n",
    "    image_c = torch.from_numpy(image_c)\n",
    "\n",
    "    # Normalize image\n",
    "    image_c = (image_c / 255.0).unsqueeze(dim=0)\n",
    "    image_c = normalize(image_c, mean, std)\n",
    "\n",
    "    # Move data and model on the device\n",
    "    feature_extractor = feature_extractor.to(DEVICE)\n",
    "    image_c = image_c.to(DEVICE)\n",
    "\n",
    "    feature_vect = feature_extractor(image_c).flatten()\n",
    "\n",
    "    return feature_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ef6ebdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:17.936743Z",
     "iopub.status.busy": "2023-01-07T22:52:17.936411Z",
     "iopub.status.idle": "2023-01-07T22:52:17.947147Z",
     "shell.execute_reply": "2023-01-07T22:52:17.946276Z"
    },
    "papermill": {
     "duration": 0.074659,
     "end_time": "2023-01-07T22:52:17.949538",
     "exception": false,
     "start_time": "2023-01-07T22:52:17.874879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def matching(\n",
    "    fv, db, dist_metric=\"euclidean\", n_closest=1\n",
    "):\n",
    "    # Fetch feature vectors from the database\n",
    "    db_labels = np.array(list(db.keys()))\n",
    "    db_fvs = np.array(list(db.values()))\n",
    "    \n",
    "    # Compute distance between fv and all samples in the database\n",
    "    # print(\"Computing distance matrix...\")\n",
    "    distances = distance.cdist(fv[np.newaxis, :], db_fvs, metric=dist_metric)[0]\n",
    "\n",
    "    # Sort distances\n",
    "    sorted_idxs = np.argsort(distances)\n",
    "    n_closest_samples = db_labels[sorted_idxs[0:n_closest]].copy()\n",
    "    n_closest_dists = distances[sorted_idxs[0:n_closest]].copy()\n",
    "    \n",
    "    del db_labels\n",
    "    del db_fvs\n",
    "    \n",
    "    return n_closest_samples, n_closest_dists\n",
    "\n",
    "\n",
    "def rec_pipeline(\n",
    "    test_image, seg_model, lbp_descriptor, resnet_model=None, \n",
    "    db=None, n_closest=1, dist_metric=\"euclidean\"\n",
    "):\n",
    "    \n",
    "    closest_samples, closest_dists = None, None\n",
    "    \n",
    "    # Segment ear from the image\n",
    "    cropped_image = segment_ear(seg_model, test_image, THRESHOLD)\n",
    "    #print(np.array(cropped_image).shape)\n",
    "    \n",
    "    if not cropped_image:\n",
    "        return closest_samples, closest_dists\n",
    "    \n",
    "    # Convert to gray scale\n",
    "    cropped_gray_image = np.array(cropped_image.convert(\"L\"))\n",
    "    \n",
    "    # Extract local binary patterns\n",
    "    fv = lbp_descriptor.describe(cropped_gray_image[np.newaxis, :, :], tile_width=TILE_WIDTH)[0]\n",
    "    \n",
    "    # ResNet50 features\n",
    "    if resnet_model:\n",
    "        resnet_fv = resnet_50_features(resnet_model, cropped_image).cpu()\n",
    "        fv = np.concatenate([fv, resnet_fv])\n",
    "    \n",
    "    # Find the closest samples in the database\n",
    "    closest_samples, closest_dists = matching(fv, db, dist_metric, n_closest)\n",
    "    \n",
    "    return closest_samples, closest_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb10149b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:18.074209Z",
     "iopub.status.busy": "2023-01-07T22:52:18.073879Z",
     "iopub.status.idle": "2023-01-07T22:52:18.080396Z",
     "shell.execute_reply": "2023-01-07T22:52:18.079486Z"
    },
    "papermill": {
     "duration": 0.070703,
     "end_time": "2023-01-07T22:52:18.082490",
     "exception": false,
     "start_time": "2023-01-07T22:52:18.011787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_database(database_dir):\n",
    "    database = {}\n",
    "\n",
    "    print(f\"Loading database: {database_dir}\")\n",
    "    for (dirpath, dirnames, filenames) in os.walk(database_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.pickle'):\n",
    "                full_path = os.sep.join([dirpath, filename])\n",
    "                identity = \"/\".join(full_path.strip(\".pickle\").split(\"/\")[-2:])\n",
    "                \n",
    "                with open(full_path, \"rb\") as handle:\n",
    "                    db_fv = pickle.load(handle)\n",
    "\n",
    "                database[identity] = db_fv\n",
    "                \n",
    "    return database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48d1e5",
   "metadata": {
    "papermill": {
     "duration": 0.062477,
     "end_time": "2023-01-07T22:52:18.204726",
     "exception": false,
     "start_time": "2023-01-07T22:52:18.142249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d24fc7",
   "metadata": {
    "papermill": {
     "duration": 0.063712,
     "end_time": "2023-01-07T22:52:18.328545",
     "exception": false,
     "start_time": "2023-01-07T22:52:18.264833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Make sure than non of the test samples are accidentally in the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2660b9d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:52:18.453314Z",
     "iopub.status.busy": "2023-01-07T22:52:18.452942Z",
     "iopub.status.idle": "2023-01-07T22:53:10.366522Z",
     "shell.execute_reply": "2023-01-07T22:53:10.365531Z"
    },
    "papermill": {
     "duration": 51.978273,
     "end_time": "2023-01-07T22:53:10.368968",
     "exception": false,
     "start_time": "2023-01-07T22:52:18.390695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database: /kaggle/input/ear-features/lbp_features1\n"
     ]
    }
   ],
   "source": [
    "LBP_FEATURES_DIR = \"/kaggle/input/ear-features/lbp_features1\"\n",
    "db = load_database(LBP_FEATURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37d26899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:53:10.494613Z",
     "iopub.status.busy": "2023-01-07T22:53:10.494233Z",
     "iopub.status.idle": "2023-01-07T22:54:49.274795Z",
     "shell.execute_reply": "2023-01-07T22:54:49.273761Z"
    },
    "papermill": {
     "duration": 98.844883,
     "end_time": "2023-01-07T22:54:49.276974",
     "exception": false,
     "start_time": "2023-01-07T22:53:10.432091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [01:38<00:00, 30.93it/s]\n"
     ]
    }
   ],
   "source": [
    "db_labels = np.array(list(db.keys()))\n",
    "\n",
    "for _, (_, test_label) in enumerate(tqdm(test_dataset)):\n",
    "    if test_label in list(db.keys()):\n",
    "        raise Expection(f\"test_label: {test_label} in the database !!!\")\n",
    "\n",
    "del db_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42ab1a",
   "metadata": {
    "papermill": {
     "duration": 0.096498,
     "end_time": "2023-01-07T22:54:49.473358",
     "exception": false,
     "start_time": "2023-01-07T22:54:49.376860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Random Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d641c6de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:54:49.659085Z",
     "iopub.status.busy": "2023-01-07T22:54:49.658702Z",
     "iopub.status.idle": "2023-01-07T22:54:49.664078Z",
     "shell.execute_reply": "2023-01-07T22:54:49.663136Z"
    },
    "papermill": {
     "duration": 0.100583,
     "end_time": "2023-01-07T22:54:49.666144",
     "exception": false,
     "start_time": "2023-01-07T22:54:49.565561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_model(identities, n_closest=1):\n",
    "    closest_samples = np.random.choice(list(identities.keys()), n_closest, replace=False)\n",
    "    return closest_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27ce4683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:54:49.849808Z",
     "iopub.status.busy": "2023-01-07T22:54:49.849438Z",
     "iopub.status.idle": "2023-01-07T22:56:03.935418Z",
     "shell.execute_reply": "2023-01-07T22:56:03.932539Z"
    },
    "papermill": {
     "duration": 74.181349,
     "end_time": "2023-01-07T22:56:03.937953",
     "exception": false,
     "start_time": "2023-01-07T22:54:49.756604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [01:14<00:00, 41.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1[%]: 0.1309328968903437\n",
      "rank 5[%]: 0.7201309328968903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rank_1_count, rank_5_count = 0, 0\n",
    "N_CLOSEST = 5\n",
    "\n",
    "for idx, (test_image, test_label) in enumerate(tqdm(test_dataset)):\n",
    "    closest_samples = random_model(db, n_closest=N_CLOSEST)\n",
    "    \n",
    "    closest_identities = [str(cs.split('/')[0]) for cs in closest_samples]\n",
    "    actual_identitiy = test_label.split(\"/\")[0]\n",
    "        \n",
    "    if actual_identitiy in closest_identities:\n",
    "        rank_5_count += 1\n",
    "            \n",
    "    if actual_identitiy == closest_identities[0]:\n",
    "        rank_1_count += 1\n",
    "\n",
    "rank_1 = 100 * (rank_1_count / len(test_dataset))\n",
    "rank_5 = 100 * (rank_5_count / len(test_dataset))\n",
    "\n",
    "print(f\"rank 1[%]: {rank_1}\")\n",
    "print(f\"rank 5[%]: {rank_5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb052b",
   "metadata": {
    "papermill": {
     "duration": 0.11486,
     "end_time": "2023-01-07T22:56:04.175478",
     "exception": false,
     "start_time": "2023-01-07T22:56:04.060618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Recognition using LBP feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2d2e687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:56:04.410876Z",
     "iopub.status.busy": "2023-01-07T22:56:04.409945Z",
     "iopub.status.idle": "2023-01-07T22:56:04.415729Z",
     "shell.execute_reply": "2023-01-07T22:56:04.414689Z"
    },
    "papermill": {
     "duration": 0.125802,
     "end_time": "2023-01-07T22:56:04.417996",
     "exception": false,
     "start_time": "2023-01-07T22:56:04.292194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.25\n",
    "\n",
    "NUM_POINTS = 8\n",
    "RADIUS = 1\n",
    "TO_HIST = True\n",
    "TILE_WIDTH = 16\n",
    "lbp_descriptor = ScikitLBP(num_points=NUM_POINTS, radius=RADIUS, to_hist=TO_HIST, method=\"default\")\n",
    "\n",
    "LBP_FEATURES_DIR = \"/kaggle/input/ear-features/lbp_features1\"\n",
    "\n",
    "# db = load_database(LBP_FEATURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f12b1705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T22:56:04.649757Z",
     "iopub.status.busy": "2023-01-07T22:56:04.649407Z",
     "iopub.status.idle": "2023-01-07T23:30:19.756716Z",
     "shell.execute_reply": "2023-01-07T23:30:19.755358Z"
    },
    "papermill": {
     "duration": 2055.226995,
     "end_time": "2023-01-07T23:30:19.759560",
     "exception": false,
     "start_time": "2023-01-07T22:56:04.532565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [34:15<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1[%]: 12.962356792144025\n",
      "rank 5[%]: 21.374795417348608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_CLOSEST = 5\n",
    "rank_1_count, rank_5_count = 0, 0\n",
    "seg_model_fails = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (test_image, test_label) in enumerate(tqdm(test_dataset)):\n",
    "        closest_samples, closest_dists = rec_pipeline(test_image, deeplabv3_model, lbp_descriptor, \n",
    "                                                      resnet_model=None, db=db, n_closest=N_CLOSEST)\n",
    "        \n",
    "        if closest_samples is None:\n",
    "            #print(\"Segmentation model failed !\")\n",
    "            seg_model_fails += 1\n",
    "            continue\n",
    "        \n",
    "        closest_identities = [str(cs.split('/')[0]) for cs in closest_samples]\n",
    "        actual_identitiy = test_label.split(\"/\")[0]\n",
    "        \n",
    "        #print(f\"closest_identities: {closest_identities}\")\n",
    "        #print(f\"actual_identitiy: {actual_identitiy}\")\n",
    "        \n",
    "        if actual_identitiy in closest_identities:\n",
    "            rank_5_count += 1\n",
    "            \n",
    "        if actual_identitiy == closest_identities[0]:\n",
    "            rank_1_count += 1\n",
    "                                    \n",
    "rank_1 = 100 * (rank_1_count / len(test_dataset))\n",
    "rank_5 = 100 * (rank_5_count / len(test_dataset))\n",
    "\n",
    "print(f\"rank 1[%]: {rank_1}\") # 12\n",
    "print(f\"rank 5[%]: {rank_5}\") # 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47b272",
   "metadata": {
    "papermill": {
     "duration": 0.322607,
     "end_time": "2023-01-07T23:30:20.361059",
     "exception": false,
     "start_time": "2023-01-07T23:30:20.038452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Recognition using LBP and ResNet50 (pretrained on ImageNet) feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef6c4c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T23:30:20.905959Z",
     "iopub.status.busy": "2023-01-07T23:30:20.905595Z",
     "iopub.status.idle": "2023-01-07T23:31:23.456037Z",
     "shell.execute_reply": "2023-01-07T23:31:23.454893Z"
    },
    "papermill": {
     "duration": 62.827108,
     "end_time": "2023-01-07T23:31:23.458880",
     "exception": false,
     "start_time": "2023-01-07T23:30:20.631772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database: /kaggle/input/ear-features/lbp_resnet_features1\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.25\n",
    "\n",
    "NUM_POINTS = 8\n",
    "RADIUS = 1\n",
    "TO_HIST = True\n",
    "TILE_WIDTH = 16\n",
    "lbp_descriptor = ScikitLBP(num_points=NUM_POINTS, radius=RADIUS, to_hist=TO_HIST, method=\"default\")\n",
    "\n",
    "LBP_RESNET_FEATURES_DIR = \"/kaggle/input/ear-features/lbp_resnet_features1\"\n",
    "\n",
    "db = load_database(LBP_RESNET_FEATURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d12f5fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T23:31:23.996258Z",
     "iopub.status.busy": "2023-01-07T23:31:23.995904Z",
     "iopub.status.idle": "2023-01-08T00:09:27.820178Z",
     "shell.execute_reply": "2023-01-08T00:09:27.819261Z"
    },
    "papermill": {
     "duration": 2284.190253,
     "end_time": "2023-01-08T00:09:27.918512",
     "exception": false,
     "start_time": "2023-01-07T23:31:23.728259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [38:03<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1[%]: 12.76595744680851\n",
      "rank 5[%]: 23.20785597381342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rank_1_count, rank_5_count = 0, 0\n",
    "seg_model_fails = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (test_image, test_label) in enumerate(tqdm(test_dataset)):\n",
    "        closest_samples, closest_dists = rec_pipeline(test_image, deeplabv3_model, lbp_descriptor, \n",
    "                                                      resnet_model=resnet50_feature_extractor,\n",
    "                                                      db=db, n_closest=N_CLOSEST)\n",
    "        \n",
    "        if closest_samples is None:\n",
    "            #print(\"Segmentation model failed !\")\n",
    "            seg_model_fails += 1\n",
    "            continue\n",
    "        \n",
    "        closest_identities = [str(cs.split('/')[0]) for cs in closest_samples]\n",
    "        actual_identitiy = test_label.split(\"/\")[0]\n",
    "        \n",
    "        #print(f\"closest_identities: {closest_identities}\")\n",
    "        #print(f\"actual_identitiy: {actual_identitiy}\")\n",
    "        \n",
    "        if actual_identitiy in closest_identities:\n",
    "            rank_5_count += 1\n",
    "            \n",
    "        if actual_identitiy == closest_identities[0]:\n",
    "            rank_1_count += 1\n",
    "                                    \n",
    "rank_1 = 100 * (rank_1_count / len(test_dataset))\n",
    "rank_5 = 100 * (rank_5_count / len(test_dataset))\n",
    "\n",
    "print(f\"rank 1[%]: {rank_1}\")\n",
    "print(f\"rank 5[%]: {rank_5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ba34f",
   "metadata": {
    "papermill": {
     "duration": 0.425202,
     "end_time": "2023-01-08T00:09:28.829094",
     "exception": false,
     "start_time": "2023-01-08T00:09:28.403892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Try out different distance metrics\n",
    "\n",
    "We'll also compute rank 1 and rank 5 using the cosine and Manhattan distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b38a0b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T00:09:29.709189Z",
     "iopub.status.busy": "2023-01-08T00:09:29.708828Z",
     "iopub.status.idle": "2023-01-08T02:54:17.888578Z",
     "shell.execute_reply": "2023-01-08T02:54:17.887604Z"
    },
    "papermill": {
     "duration": 9889.100811,
     "end_time": "2023-01-08T02:54:18.384606",
     "exception": false,
     "start_time": "2023-01-08T00:09:29.283795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database: /kaggle/input/ear-features/lbp_features1\n",
      "  mode distance metric              rank 1              rank 5\n",
      "0  LBP          cosine   14.63175122749591  23.404255319148938\n",
      "1  LBP       cityblock  19.410801963993453  29.885433715220948\n",
      "Loading database: /kaggle/input/ear-features/lbp_resnet_features1\n",
      "           mode distance metric              rank 1              rank 5\n",
      "0  LBP_RESNET50          cosine  13.486088379705402  23.011456628477905\n",
      "1  LBP_RESNET50       cityblock   16.53027823240589   29.49263502454992\n"
     ]
    }
   ],
   "source": [
    "distance_metrics = [\"cosine\", \"cityblock\"]\n",
    "N_CLOSEST = 5\n",
    "\n",
    "# Iterate over lbp features\n",
    "for mode in [\"LBP\", \"LBP_RESNET50\"]:\n",
    "    if mode == \"LBP\":\n",
    "        resnet_model = None\n",
    "        db = load_database(LBP_FEATURES_DIR) \n",
    "    else:\n",
    "        resnet_model = resnet50_feature_extractor\n",
    "        db = load_database(LBP_RESNET_FEATURES_DIR)\n",
    "        \n",
    "    run_stats = []\n",
    "    \n",
    "    # Try out all distance metrics\n",
    "    for dist_metric in distance_metrics:\n",
    "        rank_1_count, rank_5_count = 0, 0\n",
    "        \n",
    "        # Iterate over test samples and compute rank 1 and rank 5.\n",
    "        with torch.no_grad():\n",
    "            for idx, (test_image, test_label) in enumerate(test_dataset):\n",
    "                # if idx == 5: break\n",
    "        \n",
    "                closest_samples, closest_dists = rec_pipeline(test_image, deeplabv3_model, lbp_descriptor, \n",
    "                                                      resnet_model=resnet_model,\n",
    "                                                      db=db, n_closest=N_CLOSEST, dist_metric=dist_metric)\n",
    "\n",
    "                if closest_samples is None:\n",
    "                    # print(\"Segmentation model failed !\")\n",
    "                    continue\n",
    "\n",
    "                closest_identities = [str(cs.split('/')[0]) for cs in closest_samples]\n",
    "                actual_identitiy = test_label.split(\"/\")[0]\n",
    "\n",
    "                if actual_identitiy in closest_identities:\n",
    "                    rank_5_count += 1\n",
    "\n",
    "                if actual_identitiy == closest_identities[0]:\n",
    "                    rank_1_count += 1\n",
    "\n",
    "        rank_1 = 100 * (rank_1_count / len(test_dataset))\n",
    "        rank_5 = 100 * (rank_5_count / len(test_dataset))\n",
    "        run_stats.append([mode, dist_metric, rank_1, rank_5])\n",
    "        \n",
    "    # Create a pandas dataframe and save it\n",
    "    run_stats_df = pd.DataFrame(np.array(run_stats), columns=[\"mode\", \"distance metric\", \"rank 1\", \"rank 5\"])\n",
    "    print(run_stats_df)\n",
    "    run_stats_df.to_csv(f\"./{mode}_run_stats.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14569.747182,
   "end_time": "2023-01-08T02:54:21.239261",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-07T22:51:31.492079",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0949b08fdf2241938647433614fc064e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_26a1d2c8fdeb4fe49d73b06c85d90a88",
        "IPY_MODEL_497513b047fb4a98b273bd98d728e156",
        "IPY_MODEL_853c59c008f74e60a853549ce7409c3c"
       ],
       "layout": "IPY_MODEL_dc68ea62d5174a12bdeab4ec8eb06e32"
      }
     },
     "0dfd6034ad1f42a2bf7a76262b7217cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1544362890cf4e10bb44818d51281905": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ce8fcf13cf34c348ee72dbaaa447ff5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24ab033e86064234b51c66378a9630e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "26a1d2c8fdeb4fe49d73b06c85d90a88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_30c42210c040400b9c23bbcbf259f21b",
       "placeholder": "​",
       "style": "IPY_MODEL_0dfd6034ad1f42a2bf7a76262b7217cf",
       "value": "100%"
      }
     },
     "30c42210c040400b9c23bbcbf259f21b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dca0a36be304315a1b9497cfe3d0d5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ce8fcf13cf34c348ee72dbaaa447ff5",
       "max": 168312152,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_24ab033e86064234b51c66378a9630e2",
       "value": 168312152
      }
     },
     "497513b047fb4a98b273bd98d728e156": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1544362890cf4e10bb44818d51281905",
       "max": 102530333,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_77e6c74af6c34dba81c63b725d796aa6",
       "value": 102530333
      }
     },
     "6288decaa1524a2f8ff9417fda6caccb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b2a33486c45452c8fd1977180931888": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "70a2477b9a9a4a3dad70eb9afe798e62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "77e6c74af6c34dba81c63b725d796aa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "853c59c008f74e60a853549ce7409c3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9d71c8963ffa476297c59cfe54420994",
       "placeholder": "​",
       "style": "IPY_MODEL_aff35a84a0fc410bad7657162e4598ca",
       "value": " 97.8M/97.8M [00:03&lt;00:00, 32.5MB/s]"
      }
     },
     "981cc97322074adea667bbd2557b74b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d71c8963ffa476297c59cfe54420994": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aff35a84a0fc410bad7657162e4598ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c78718b118f446e5b309bb1437d341df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d3b42c5782fb477186652eef9655466f",
        "IPY_MODEL_3dca0a36be304315a1b9497cfe3d0d5b",
        "IPY_MODEL_d7ee8594f44d4b4c9e6805ca25aa0d13"
       ],
       "layout": "IPY_MODEL_c85234738d0347f0a23bb71fcb139acb"
      }
     },
     "c85234738d0347f0a23bb71fcb139acb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3b42c5782fb477186652eef9655466f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6288decaa1524a2f8ff9417fda6caccb",
       "placeholder": "​",
       "style": "IPY_MODEL_70a2477b9a9a4a3dad70eb9afe798e62",
       "value": "100%"
      }
     },
     "d7ee8594f44d4b4c9e6805ca25aa0d13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_981cc97322074adea667bbd2557b74b6",
       "placeholder": "​",
       "style": "IPY_MODEL_6b2a33486c45452c8fd1977180931888",
       "value": " 161M/161M [00:05&lt;00:00, 32.7MB/s]"
      }
     },
     "dc68ea62d5174a12bdeab4ec8eb06e32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
